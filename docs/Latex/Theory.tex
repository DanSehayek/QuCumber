% =========================================================================
% SciPost LaTeX template
% Version 1e (2017-10-31)
%
% Submissions to SciPost Journals should make use of this template.
%
% INSTRUCTIONS: simply look for the `TODO:' tokens and adapt your file.
%
% - please enable line numbers (package: lineno)
% - you should run LaTeX twice in order for the line numbers to appear
% =========================================================================


% TODO: uncomment ONE of the class declarations below
% If you are submitting a paper to SciPost Physics: uncomment next line
\documentclass[submission, Phys]{SciPost}
% If you are submitting a paper to SciPost Physics Lecture Notes: uncomment next line
%\documentclass[submission, LectureNotes]{SciPost}
% If you are submitting a paper to SciPost Physics Proceedings: uncomment next line
%\documentclass[submission, Proceedings]{SciPost}

\usepackage{braket}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, bm}
\usepackage{dsfont}
\usepackage{pythonhighlight}
\SetKwComment{Comment}{$\triangleright$\ }{}


\begin{document}

% TODO: write your article's title here.
% The article title is centered, Large boldface, and should fit in two lines
\begin{center}{\Large \textbf{
QuCumber: neural network based generative modelling for quantum wavefunction reconstruction
}}\end{center}

% TODO: write the author list here. Use initials + surname format.
% Separate subsequent authors by a comma, omit comma at the end of the list.
% Mark the corresponding author with a superscript *.
\begin{center}
PIQuIL people\textsuperscript{1},
in alphabetical order\textsuperscript{2},
I.C.Weiner\textsuperscript{3*}
\end{center}

% TODO: write all affiliations here.
% Format: institute, city, country
\begin{center}
{\bf 1} Affiliation1
\\
{\bf 2} Affiliation2
\\
{\bf 3} Affiliation2
\\
% TODO: provide email address of corresponding author
* CorrespondingAuthor@email.address
\end{center}

\begin{center}
\today
\end{center}

% For convenience during refereeing: line numbers
%\linenumbers

\section*{Abstract}
{\bf
% TODO: write your abstract here.
The abstract is in boldface, and should fit in 8 lines.
It should be written in a clear and accessible style, emphasizing the context, the problem(s) studied, the methods used, the results obtained, the conclusions reached, and the outlook. You can add a table contents, recommended if your paper is more than 6 pages long.

In this post we present a new open source Python package for quantum state tomography with restricted Boltzmann machines, called QuCumber. The use of machine learning makes it possible to efficiently learn quantum states and reconstruct and access traditionally challenging many-body quantities from experimental measurements. We give examples how to use QuCumber on positive-real and complex wavefunctions and how to extract meaningful observables such as energy, magnetization and fidelity.
}


% TODO: include a table of contents (optional)
% Guideline: if your paper is longer that 6 pages, include a TOC
% To remove the TOC, simply cut the following block
\vspace{10pt}
\noindent\rule{\textwidth}{1pt}
\tableofcontents\thispagestyle{fancy}
\noindent\rule{\textwidth}{1pt}
\vspace{10pt}


%\section{What can I use this RBM Quantum State Reconstruction Library for?}
\section{Quantum State Reconstruction}

Scientific progress on controlling quantum systems is producing increasingly large devices that exhibit highly pure quantum states.  As the size of these devices grow to tens or hundreds of qubits, we are faced with the task of characterizing and understanding the nature of the many-body quantum state, based on a finite set of experimental measurements.  As the computational challenge grows rapidly with qubit number, the community is increasingly exploring the ability of modern machine learning techniques to aid in the characterization of near-term, highly-controlled quantum devices.

Generative modelling with stochastic neural networks offers a highly promising, scalable approximation technique for learning quantum wavefunctions given synthetic or experimental measurement data.  The rapid development of machine learning algorithms for industrial applications has produced technology highly specialized in its ability to provide efficient techniques for fitting model parameters to mimic the probability distribution underlying a set of data.  One of the most successful generative models is the Restricted Boltzmann Machine (RBM).  RBMs have been shown to capture the essential properties of many-body systems and to be an effective tool for reconstructing quantum states from data \cite{Torlai2016thermo, torlai2018tomography, CarleoTroyer2017Science}.

Here we outline a Python package called QuCumber: a Quantum Calculator Used for Many-Body Eigenstate Reconstruction.  QuCumber is a freely accessible, open source software package that implements an RBM and trains it on binary experimental data representing measurements of qubit eigenvalues, spin states, orbital occupation numbers, etc.  Once trained, the RBM parameters encode a compressed version of the underlying wavefunction.  New quantum state measurements can then be generated by the RBM, and some observables not otherwise easily accessible from the original data can be straightforwardly calculated.

This technique has been used in the past to reconstruct wavefunctions from theoretical and experimental data, and to produce non-trivial measurements such as off-diagonal correlation functions and entanglement entropies.
In this paper, we will outline how QuCumber can be used for these an other tasks in characterizing quantum many-body phenomenon on hundreds or thousands of qubits.


\subsection{A Quick Primer on Restricted Boltzmann Machines}

QuCumber allows one to reconstruct an approximate wavefunction $\psi( \boldsymbol{\sigma} )  = \langle \Psi \vert \boldsymbol{\sigma} \rangle$ in some computational basis $\{ \vert \boldsymbol{ \sigma} \rangle \}$ from experimental data.   For concreteness, imagine that the data consists of a series of qubit measurements $\vert {\boldsymbol{ \sigma}} \rangle = \vert { \sigma}_1~{ \sigma}_2 \dots \rangle$ obtained in the computational basis. If the state can be prepared (and the bit-string measurement can be produced) many times, then one can build a data set of measurements useful to train a RBM.

The simplest scenario is when the experimental wavefunction's expansion coefficients are all positive in the computational basis.  
Then the measurements will be distributed according to a probability distribution defied by Born's rule: $q(\boldsymbol{\sigma}) = | \psi( \boldsymbol{\sigma} ) |^2$.
The goal of the QuCumber is to learn the best possible approximation to the target distribution $q(\boldsymbol{\sigma})$, and to encode it in the parameters of an RBM -- a graphical representation of a probability distribution. Later in this post? we show that QuCumber is not restricted to positive and real wavefunctions.

After the data set is used to train the parameters of the RBM, it becomes a compressed reconstruction of the original experimental wavefunction.  The RBM can then be examined, characterized, and measured.  Most importantly, new quantum states can be generated by the RBM, and new observables measured.  QuCumber implements a simple framework for sampling the reconstructed wavefuntion, and performing measurements of physical observables on the generated samples.

Below we provide a starting point for writing code in QuCumber.  However, there is much more theory to learn on the physics of wavefunction reconstruction with RBMs. For the necessary background, please consult the relevant scientific literature, in particular Refs.~\cite{Torlai2016thermo, torlai2018tomography, CarleoTroyer2017Science}, and the theoretical tutorial provided in the documentation for QuCumber.  We begin with a short glossary of terms that we will refer to in the remainder of the document.

\subsection{A Glossary of Terms}

\begin{itemize}

\item {\it Biases}.  For a visible unit $v_i$ and a hidden unit $h_j$, the respective biases in the RBM energy are $b_i$ and $c_j$. They act like a magnetic field term in the energy Eq~(\ref{RBMenergy}).

\item {\it Energy}.  In analogy to statistical physics the energy of an RBM is defined given the joint configuration $(v,h)$ of visible and hidden units:
\begin{equation}
E_{\lambda}(v,h) = - \sum\limits_{i=1}^V b_i v_i - \sum\limits_{j=1}^H c_j h_j - \sum\limits_{ij} v_i W_{ij} h_j, \label{RBMenergy} 
\end{equation}

\item {\it Effective energy}.  Energy traced over the hidden units $h$:
\begin{equation}
\varepsilon_{\lambda}(v) = - \sum\limits_{i=1}^V b_i v_i - \sum\limits_{j=1}^H \log \left\{ 1 + \exp \left( \sum\limits_{i} v_i W_{ij} \right) \right\}, \label{RBMeffectiveenergy} 
\end{equation}

\item {\it Hidden Units}.  There are $H$ units in the second layer of the RBM, denoted by the vector $h=(h_1, ..., h_H)$, representing latent variables and are referred to as ``hidden".  The number of hidden units $H$ can be adjusted to tune the representational capacity of the RBM.

\item {\it Joint distribution}.  The RBM assigns a probability to each joint configuration $(v,h)$ according to the Boltzmann distribution of the energy,
\begin{equation}
    p_{\lambda}(v,h) = \frac{1}{Z} e^{-E_{\lambda}(v,h)},
\end{equation}

\item {\it Marginal distribution}.  Obtained by marginalizing the joint distribution, e.g.
\begin{equation}
\label{Eq:marginal_distribution}
    p_{\lambda}(v) = \frac{1}{Z} \sum\limits_{h\in \mathcal{H}} e^{-E_{\lambda}(v,h)} = \frac{1}{Z} e^{- \varepsilon_{\lambda}(v)}.
\end{equation}

\item {\it QuCumber}. A quantum calculator used for many-body eigenstate reconstruction.

\item {\it Parameters}.  An RBM's energy is defined via a set of neural network parameters $\lambda = \{b,c,W\}$, consisting of weights and biases.

\item {\it Partition function}. The normalizing constant of the Boltzmann distribution.  It is obtained by summing over all possible pairs of visible and hidden vectors,
\begin{equation}
    Z = \sum\limits_{v\in \mathcal{V}}\sum\limits_{h\in \mathcal{H}} e^{-E_{\lambda}(v,h)}.
\end{equation}

\item {\it Restricted Boltzmann Machine}.  A two-layer network with bidirectionally connected stochastic processing units.  ``Restricted" refers to the connections (or weights) between the visible and hidden units. Each visible unit is connected with each hidden unit, but there are no intra-layer connections.

\item {\it Visible Units}.  There are $V$ units in the first layer of the RBM, denoted by the vector $v=(v_1, ..., v_V)$, which correspond to the experimental data and are therefore called ``visible".  The number of visible units $V$ is fixed to the number of physical qubits.

\item {\it Weights}.  $W_{ij}$ is the symmetric connection or interaction between the visible unit $v_i$ and the hidden unit $h_j$.

\end{itemize}



\section{How to use QuCumber}

Below, we give examples of how to use QuCumber for typical cases in the reconstruction of quantum wavefunctions.  The following code snippets are written in Python 3, using PyTorch with CPU and GPU support.  We refer the reader to the full code documentation (\url{https://piquil.github.io/QuCumber/}) for further information on the code structure and operation.

\subsection{Training QuCumber for real-positive wavefunctions}

If all the measurements have been performed in the same basis and the experimenter assumes that the wave function is positive and real, the QuCumber package provides a more efficient tomography tool for this case. The wavefunction in the reference basis $\{ \bm{\sigma} \}$ can be approximated by the marginal distribution (Equation \ref{Eq:marginal_distribution}) $\psi_{\bm{\lambda}}(\bm{\sigma}) = \sqrt{p_{\lambda} ( \bm{\sigma})}$. Since there are no unitaries that have to be applied on the measurements, this approach is highly parallelizable  and therefore faster.
First we load all the packages needed. \textbf{We might have to rename rbm to something like NNQuST} 

\begin{python}
from rbm import RBM #import NNQuST package
import numpy as np #generic math function
import torch #ML package
\end{python}

The input data needs to be in a numpy array or a torch tensor. E.g. for a spin system with 10 physical spins and measurements of every spin one input data point will be an array of the form \verb|np.array([1,0,1,1,0,1,0,0,0,1])|, with shape \verb|(10,)|. all the input data together needs to be an array of these arrays, which will have the shape \verb|(N,10)|. Where N is the number of data points in the training set.

For a dummy test we can define a training set of a thousand (\verb|N=1000|) spin measurements, where the spins point in the same direction \verb|np.array([1,1,1,1,1,1,1,1,1,1])|:

\begin{python}
train_set = np.array([[1]*10]*1000) #define a dummy training set with the correct input shape
\end{python}

To run the RBM training we define the model parameters. The number of visible units (\verb|num_visible|) is given by the length of one training sample. (In our case it is $10$). 
The number of hidden units (\verb|num_hidden|) can be varied and the success of the training depends on the complexity of the system and the ratio $\alpha$ between hidden and visible units. It has been shown, that already $\alpha = 1$ leads to good approximations of positive real wavefunctions \cite{Torlai2016thermo}.
\textbf{refer to our scaling study?} 
The number of epochs strongly depends on the complexity of the system and also the training set size. The success of training can be tracked by different measures, like the convergence of the energy or the fidelity with a target state (For more details see section \ref{Sec:Observables}. The later is only tractable for small systems. A batch size of 32 is a good average value, which can be slightly changed without having big effects on the training success. The number of Gibbs sampling steps influences the speed of the training. The smaller we choose $k$, the faster is the training. High values on the other side give a better approximation for the model distribution. But it has been shown, that already for $k=1$ the RBM trains well \cite{hinton2002training}.

\begin{python}
num_visible = 10 #number of visible units (has to be equal to input data dimension)
num_hidden = 10 #number of hidden units
epochs = 1000 #number of epochs for the training
batch_size = 32 #batch size for the training
k = 1 #Number of Gibbs sampling steps
learning_rate = 0.01 #Learning Rate
\end{python}

\textbf{take learning rate out here eventually and put it later, when we discuss the callbacks.}

\begin{python}
rbm = RBM(num_visible=num_visible, num_hidden=num_hidden)

rbm.train(train_set, epochs, batch_size, k=k, lr=learning_rate)
\end{python}

This will already train the RBM and eventually approximate the target wavefunction. 




\subsection{Saving, Loading the QuCumber}

To save and load the trained Tomography one can run the following code.

\begin{python}
location = 'some_folder/filename'
rbm.save(location) #save the weights and biases to some location
rbm.load(location) #load the weights and biases from some location
\end{python}

\section{What can I do with a trained RBM?}

\subsection{Sampling}

Since RBMs are generative models, one can sample new measurements from a trained RBM.

\begin{python}
num_samples = 100 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
rbm.sample(num_samples, k)
\end{python}

\subsection{Observables}
\label{Sec:Observables}

Observables like energy, magnetisation, etc. can be calculated directly on the sampled data from the RBM. To get a good average value one has to draw a big enough number $N$ of samples $\bm{s}$, such that the value converges. For a general observable $A$, we calculate,

\begin{equation}
\braket{A} = \frac{1}{N} \sum_{\bm{\sigma}} A(\bm{\sigma})
\end{equation}

\subsubsection{Example1: Magnetization of Ising chain}

The magnetization of an Ising chain can be calculated simply by averaging over all $\sigma_z$ measurements of the chain. Since the RBM only can sample in the $\sigma_z$ direction, we simply have to average over the sample $\bm{\sigma}$. The only additional step that has to be performed is the convergence from binary values $\{0,1  \}$ to actual spin measurements $\{-1 ,1  \}$.

\begin{python}
num_samples = 10000 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
samples = rbm.sample(num_samples, k)
samples = (samples-1/2)*2 # converge from binary to [-1, 1]
magnetization = 0 #initialize magnetization as 0
for s in samples: 
	magnetization += s.sum(0) # sum up every sample separately
magnetization /= num_samples # devide by number of samples
\end{python}

\subsubsection{Example2: Energy of Transverse Field Ising chain}

For the transverse field Ising model the Hamiltonian reads:

\begin{equation}
H = \sum_i \sigma^z_i \sigma^z_{i+1} + h \sum_i \sigma^x_{i}
\end{equation}

Calculating the energy for a sample $\ket{\bm{\sigma} }= \ket{\sigma_1 \dots  \sigma_n}$ is not that straight forward like for the magnetization, because the Hamiltonian does not preserve the basis state, the $\sigma^x_i$ part of the Hamiltonian flips the spin at position $i$. So just calculating $\bra{\bm{\sigma} } H \ket{\bm{\sigma} }$ ignores the second part of the Hamiltonian completely. So what we have to do is the following. First we expand the wavefunction with its basis $\ket{\psi} = \sum_{\bm{\sigma}} \psi(\bm{\sigma}) \ket{\bm{\sigma}} $ and calculate the expectation value of the Hamiltonian the following way:

\begin{align}
\frac{\braket{\psi | H | \psi}}{\braket{\psi | \psi}} &= \frac{\sum_{\bm{\sigma}, \bm{\sigma'}} \psi(\bm{\sigma}) \psi (\bm{\sigma}') H_{\bm{\sigma}, \bm{\sigma}'} \cdot \psi(\bm{\sigma}) / \psi(\bm{\sigma})}{ \sum_{\bm{\sigma}} | \psi(\bm{\sigma})|^2}  \\
&=  \frac{\sum_{\bm{\sigma}} | \psi({\bm{\sigma}}) |^2 \left[ \sum_i \sigma_i \sigma_{i+1} + h \sum_i \frac{\psi(\sigma_1, \sigma_2, \dots , -\sigma_i, \sigma_{i+1} \dots)}{\psi (\bm{\sigma})} \right]}{\sum_{\bm{\sigma}} | \psi({\bm{\sigma}} )|^2} \\
&\approx \frac{1}{M} \sum_k^M \left[ \sum_i \sigma_i \sigma_{i+1} + h \sum_i \frac{\psi (\bm{\sigma}_{-i})}{\psi (\bm{\sigma})} \right]
\end{align}

Here we first expanded the expectation value with the factor $\psi(\bm{\sigma}) / \psi(\bm{\sigma})$ and then used that $H_{\bm{\sigma}, \bm{\sigma}'}  = \braket{\bm{\sigma} | H | \bm{\sigma}'} = \sum_i \delta_{\bm{\sigma}, \bm{\sigma}'} + h \sum_i \delta_{\sigma_1, \sigma_1'} \dots \delta_{\sigma_i, -\sigma_i'} \dots$. Finally we denoted $\bm{\sigma}_{-i} = \sigma_1, \sigma_2, \dots , -\sigma_i, \sigma_{i+1} \dots$, which is the sample $\bm{\sigma}$ with the i-th spin flipped.
The expectation value can be approximated by drawing $M$ samples and averaging over them.

The sum $\sum_i \sigma_i \sigma_{i+1}$ simply multiplies the neighbouring elements of the sample $\bm{\sigma}$. For the second sum one actually has to calculate the probabilities $\psi (\bm{\sigma})$ of the sample $\bm{\sigma}$ and $\bm{\sigma}_{-i} $ with

\begin{equation}
\psi(\bm{\sigma}) = \sqrt{p_{\lambda}(\bm{\sigma})}
\end{equation}

with $p_{\lambda}(\bm{\sigma})$ from Equation~\ref{Eq:marginal_distribution}. The partition functions $Z$ cancel out if we divide the probabilities and the calculation is tractable also for big systems.
The observables.py package contains a few examples like this.

\begin{python}
from observables import TFIMChainEnergy
num_samples = 10000 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
h = 1.0 #define magnetic field strength
samples = rbm.sample(num_samples, k)
calc_energy = TFIMChainEnergy(h)
energy = calc_energy.apply(samples, rbm)
\end{python}

\subsection{Fidelity}

If one has access to the full quantum state $\ket{\phi} = \sum_{\bm{\sigma}} \phi(\bm{\sigma}) \ket{ \bm{\sigma}}$ and knows the coefficients $\phi(\bm{\sigma)}$ that are approximated by the RBM $\psi_{\lambda}(\bm{\sigma}) = \sqrt{p_{\lambda}(v)}$ the fidelity can be computed via $\sum_{\bm{\sigma}} \phi(\bm{\sigma)} \psi_{\lambda}(\bm{\sigma})$. This can be used to verify for example that experimental data actually comes from the state that should be prepared in an experiment. Because of the exponential growth of the wave function this is only feasible for small physical systems.


The effective energy $\varepsilon$ and the unnormalized probability $p_{\lambda}(v)$ of some configuration 
\verb|torch.tensor([1,0,1,1,1,1,1,1,0,1])| can be accessed from the trained RBM via

\textbf{We need to rename free energy to effective energy in code!!!}

\begin{python}
some_configuration = torch.tensor([1,0,1,1,1,1,1,1,0,1], dtype = torch.double)
effective_energy = rbm.free_energy(some_configuration)
unnormalized_probability = rbm.unnormalized_probability(some_configuration) 
\end{python}

The partition function $Z_{\Theta}$ can be calculated by summing over all possible spin configurations, which is also the limiting factor for this approach for bigger systems.

\begin{python}
visble_space = rbm.generate_visble_space()
partition_fct = rbm.partition(visible_space)
\end{python}

To effectively calculate the fidelity with a theoretical state $\\ket{phi}$, one needs to load the coefficients $\phi(\bm{\sigma})$. In this example we create a random state with $2^N$ coefficients, where $N=10$ is the system size.

\begin{python}
N = 10 # Define system size
phi =  torch.abs(torch.randn(2**N)) # generate random vector (this would be the theoretical wave function)
phi /= phi.norm() # normalize the vector
effective_energy = rbm.free_energy(some_configuration)
unnormalized_probability = rbm.unnormalized_probability(some_configuration) 
\end{python}

\section{Improve the training accuracy of the RBM}

So far there is no certainty that the RBM performs well and indeed learns the probability distribution it is supposed to learn. Therefore we introduce the concept of callbacks and also the learning rate. The learning rate is crucial for the success of the training and has to be adjusted if the the RBM performs poorly.
To tract a certain observable like the energy or the fidelity during the training one can implement it as a class in the \verb|observable.py| file and monitor it via callbacks.

\section{Algorithm for a real positive wave function}
The training algorithm of the RBM has the following structure.
%=================================================================================
% RBM.train
 
\begin{algorithm}[H]
	 \caption{Training Algorithm of RBM. \textbf{RBM.train}() }
  \SetAlgoLined
  \For{batch in training set}{
  Load batch from training set, batch $=(\hat{ \boldsymbol{\sigma}}_1~\hat{ \boldsymbol{ \sigma}}_2 \dots)$\;
  compute the gradients from the batch $\Delta \bm{\Theta }  =$ ($\Delta W$, $\Delta b$, $\Delta c$)\\
  \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch)   \Comment*[r]{Algorithm 2}
  update weights and biases \\
$\bm{\Theta} \leftarrow \bm{\Theta } - \Delta \bm{\Theta }  $ \;
  }
 
\end{algorithm}

%=================================================================================
% RBM.compute gradient

The gradients are calculated according to the contrastive divergence algorithm, which allows us to approximate the probability distribution of the model with $k$ Gibbs sampling steps from the actual training data.

\begin{algorithm}[H]
	 \caption{Compute Gradient from Batch. \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch) }
  \SetAlgoLined
  Reset gradients $\Delta W$, $\Delta h_b$, $\Delta v_b$ $= 0$\;
  \For{$\hat{ \boldsymbol{ \sigma}}_i$ in batch}{
  sample $\bm{h}_0$, $\bm{v}_k$ and $\bm{p}_{h_k}$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
   \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$)   \Comment*[r]{Algorithm 3}
  calculate gradients\\
  $\Delta W += \bm{v}_0 \bm{h}_0^T - \bm{v}_k \bm{p}_{h_k}^T$ \\
  $\Delta c += \bm{h}_0 - \bm{p}_{h_k}$ \\
  $\Delta b += \bm{v}_0 - \bm{v}_k$ \;
  }
 $M = \vert batch \vert$ \;
 return $\Delta W / M$,  $\Delta c / M$, $\Delta b / M$
\end{algorithm}

%=================================================================================
% RBM.gibbs sampling

The Gibbs sampling is done $k$ times back and forth. The contrastive divergence algorithm already shows good results for $k=1$. But for better results this value can be increased.

\begin{algorithm}[H]
	 \caption{Gibbs sampling. \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$) }
  \SetAlgoLined
  calculate $\bm{p}_h$ and sample $\bm{h}_0$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
  \textbf{\lstinline{RBM.sample_h_given_v}}($\hat{ \boldsymbol{ \sigma}}$)   \Comment*[r]{Algorithm 4}
  $\bm{h} = \bm{h}_0$\;
  i = 0\;
  \While{i $\leq$ k}{
	calculate $\bm{p}(\bm{v}|\bm{h}_i)$ and sample $\bm{v}$ from $\bm{h}$\\
	\textbf{\lstinline{RBM.sample_v_given_h}}($\bm{h}$)   \Comment*[r]{Algorithm 5}
	calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ from $\bm{v}$\\
	\textbf{\lstinline{RBM.sample_h_given_v}}($\bm{v}$)   \Comment*[r]{Algorithm 4}
	$i +=1$
  }
  return $\bm{p}_{h_k} =$  $\bm{p}(\bm{h}|\bm{v})$, $\bm{v}_k = \bm{v}$ and $\bm{h}_0$\;
 
\end{algorithm}


%=================================================================================
% RBM.v given h and vice versa



\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{v}|\bm{h})$ and sample $\bm{v}$ \textbf{\lstinline{RBM.v_given_h}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{v} = 1|\bm{h}) = \sigma( W \bm{h} + v_b)$\;
  Bernoulli sample $\bm{v}$ from this probability\;
  return $\bm{v}$ and $\bm{p}(\bm{v} = 1|\bm{h})$\;
 
\end{algorithm}


\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ \textbf{\lstinline{RBM.h_given_v}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{h} = 1|\bm{v}) = \sigma(\bm{v}^T W  + h_b)$\;
  Bernoulli sample $\bm{h}$ from this probability\;
  return $\bm{h}$ and $\bm{p}(\bm{h} = 1|\bm{v})$\;
 
\end{algorithm}

\section{Training QuCumber for complex wavefunctions}

\textbf{MAY BE WE CAN ADD A GLOSSARY HERE FOR ALL THE COMPLEX STUFF}

For the positiv-real wavefunctions there is no additional information contained in the wavefunction $\psi( \boldsymbol{\sigma})$ that could be lost by Borns rule $q(\boldsymbol{\sigma}) = | \psi( \boldsymbol{\sigma} ) |^2$. For negative-real and complex functions the phase information contained in these coefficients will be lost if the measurement is only made in one basis. Therefore in experiments one has to apply local unitary transformations before the measurements to capture the phase information. With this additional information one can successfully perform the quantum state tomography.
QuCumber also provides the possibility to do QST for complex wavefunctions. The unitary transformations have to be provided in the following form \textbf{ADD UNITARIES EXAMPLE}

For complex wavefunctions additionally to the marginal distribution (Equation \ref{Eq:marginal_distribution} we need a phase factor that has to be learned by an additional set of hidden units and corresponding network parameters $\mu$ of the RBM

\begin{align}
\psi_{\bm{\lambda} \bm{\mu}} (\bm{\sigma})= \sqrt{p_{\lambda} (\bm{\sigma})} e^{i \phi_{\mu} (\bm{\sigma})/2},
\end{align}

where $\phi_{\mu}(\bm{\sigma}) = \log (p_{\mu} (\bm{\sigma}))$. The complex wavefunction can be rewritten in a generic basis $\{ \bm{\sigma}^b \}$as :

\begin{align}
\psi_{\bm{\lambda} \bm{\mu}} (\bm{\sigma}^b)= \sum_{\bm{\sigma}} U (\bm{\sigma}^b, \bm{\sigma}) \psi_{\bm{\lambda} \bm{\mu}} (\bm{\sigma}),
\end{align}

with the unitary $U (\bm{\sigma}^b, \bm{\sigma})$ that rotates the state $\ket{\bm{\sigma}^b}$ to $\ket{\bm{\sigma}}$. Our objective is to learn the full complex wavefunction $\psi_{\bm{\lambda} \bm{\mu}} (\bm{\sigma})$ in the reference basis (in this case the computational basis) from the measurements of the rotated wavefunction $\psi_{\bm{\lambda} \bm{\mu}} (\bm{\sigma}^b)$.
The RBM again is trained by minimizing the negative log-likelihood. Following \cite{} \textbf{ADD REFERENCE TO GIACOMOS THESIS OR SOMETHING THAT SHOWS THE MATH OF THE COMPLEX WAVEFUNCTION} the gradients of the NLL will contain the unitaries $U (\bm{\sigma}^b, \bm{\sigma})$ and therefore we have to apply a slightly different learning algorithm.

\subsection{A 2 qubit example}

In this section we guide the reader through a simple example for full state tomography of a complex wavefunction with 2 qubits. On the github repository there is a dataset with measurements, the unitaries that have been applied before the measurement and the actual wavefunction $\ket{\psi}$ the measurements have been sampled from.

Load the files the following way:

\begin{python}
load files
measurements = 
unitaries = 
psi =
\end{python}


We want to do tomography of the 2 qubit state $\ket{\psi} = 1/2 \{ \ket{00} - \ket{01} + \ket{10} - i \ket{11} \}$. If we measure this state in the computational basis we obtain all states ($00$, $01$, $10$ and $11$) with the same probability $p(m) = 1/4$, but we do not learn anything about the phase $i$ or $(-1)$. Therefore we apply local unitaries on the state before we measure in the computational basis. \verb|unitaries| contains the unitaries applied to the respective measurement and has the form  \verb|np.array(['ZZ', 'XZ', 'ZX', ...])|. Which means that the first measurement has been done in the Z basis for both qubits, therefore no unitary has been applied. The second measurement has been done in the XZ basis, which means, that the first qubit has been measured in the X basis. This can be done by applying a Hadamard gate $H$ to the first qubit before measuring it in the computational basis. For measurements in the Y basis one has to apply the $K$ matrix before measuring in Z basis.

\begin{python}
run training
\end{python}

\subsection{Add new unitaries to the dictionary}

QuCumber contains by default the unitaries $\mathds{1}$, $H$ and $K$. If the user wants to add new basis transformations one can do that the following way:

\begin{python}
from qucumber import unitary
import numpy as np
new_unitary
unitary_dict = unitaries.create_dict(name = 'new_unitary', unitary = new_unitary)
\end{python}

\section{Conclusion}
You must include a conclusion.

\section*{Acknowledgements}
Acknowledgements should follow immediately after the conclusion.

% TODO: include author contributions
\paragraph{Author contributions}
This is optional. If desired, contributions should be succinctly described in a single short paragraph, using author initials.

% TODO: include funding information
\paragraph{Funding information}
Authors are required to provide funding information, including relevant agencies and grant numbers with linked author's initials. Correctly-provided data will be linked to funders listed in the \href{https://www.crossref.org/services/funder-registry/}{\sf Fundref registry}.


\begin{appendix}

\section{How to install QuCumber}

\section{Complete example codes}

\section{About references}
Your references should start with the comma-separated author list (initials + last name), the publication title in italics, the journal reference with volume in bold, start page number, publication year in parenthesis, completed by the DOI link (linking must be implemented before publication). If using BiBTeX, please use the style files provided  on \url{https://scipost.org/submissions/author_guidelines}. If you are using our \LaTeX template, simply add
\begin{verbatim}
\bibliography{your_bibtex_file}
\end{verbatim}
at the end of your document. If you are not using our \LaTeX template, please still use our bibstyle as
\begin{verbatim}
\bibliographystyle{SciPost_bibstyle}
\end{verbatim}
in order to simplify the production of your paper.
\end{appendix}


% TODO:
% Provide your bibliography here. You have two options:

% FIRST OPTION - write your entries here directly, following the example below, including Author(s), Title, Journal Ref. with year in parentheses at the end, followed by the DOI number.
%\begin{thebibliography}{99}
%\bibitem{1931_Bethe_ZP_71} H. A. Bethe, {\it Zur Theorie der Metalle. i. Eigenwerte und Eigenfunktionen der linearen Atomkette}, Zeit. f{\"u}r Phys. {\bf 71}, 205 (1931), \doi{10.1007\%2FBF01341708}.
%\bibitem{arXiv:1108.2700} P. Ginsparg, {\it It was twenty years ago today... }, \url{http://arxiv.org/abs/1108.2700}.
%\end{thebibliography}

% SECOND OPTION:
% Use your bibtex library
% \bibliographystyle{SciPost_bibstyle} % Include this style file here only if you are not using our template
\bibliography{bibliography}

\nolinenumbers

\end{document}
