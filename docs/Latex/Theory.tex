% =========================================================================
% SciPost LaTeX template
% Version 1e (2017-10-31)
%
% Submissions to SciPost Journals should make use of this template.
%
% INSTRUCTIONS: simply look for the `TODO:' tokens and adapt your file.
%
% - please enable line numbers (package: lineno)
% - you should run LaTeX twice in order for the line numbers to appear
% =========================================================================


% TODO: uncomment ONE of the class declarations below
% If you are submitting a paper to SciPost Physics: uncomment next line
\documentclass[submission, Phys]{SciPost}
% If you are submitting a paper to SciPost Physics Lecture Notes: uncomment next line
%\documentclass[submission, LectureNotes]{SciPost}
% If you are submitting a paper to SciPost Physics Proceedings: uncomment next line
%\documentclass[submission, Proceedings]{SciPost}

\usepackage{braket}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{amsmath, bm}
\usepackage{pythonhighlight}
\SetKwComment{Comment}{$\triangleright$\ }{}


\begin{document}

% TODO: write your article's title here.
% The article title is centered, Large boldface, and should fit in two lines
\begin{center}{\Large \textbf{
QuCumber: a neural-network based generative model for wavefunction reconstruction
}}\end{center}

% TODO: write the author list here. Use initials + surname format.
% Separate subsequent authors by a comma, omit comma at the end of the list.
% Mark the corresponding author with a superscript *.
\begin{center}
PIQuIL people\textsuperscript{1},
in alphabetical order\textsuperscript{2},
I.C.Weiner\textsuperscript{3*}
\end{center}

% TODO: write all affiliations here.
% Format: institute, city, country
\begin{center}
{\bf 1} Affiliation1
\\
{\bf 2} Affiliation2
\\
{\bf 3} Affiliation2
\\
% TODO: provide email address of corresponding author
* CorrespondingAuthor@email.address
\end{center}

\begin{center}
\today
\end{center}

% For convenience during refereeing: line numbers
%\linenumbers

\section*{Abstract}
{\bf
% TODO: write your abstract here.
The abstract is in boldface, and should fit in 8 lines.
It should be written in a clear and accessible style, emphasizing the context, the problem(s) studied, the methods used, the results obtained, the conclusions reached, and the outlook. You can add a table contents, recommended if your paper is more than 6 pages long.
}


% TODO: include a table of contents (optional)
% Guideline: if your paper is longer that 6 pages, include a TOC
% To remove the TOC, simply cut the following block
\vspace{10pt}
\noindent\rule{\textwidth}{1pt}
\tableofcontents\thispagestyle{fancy}
\noindent\rule{\textwidth}{1pt}
\vspace{10pt}


\section{What can I use this RBM Quantum State Tomography Library for?}

Quantum state tomography is used to describe an unknown quantum state by performing measurements on the state. It is known, that for a full description of an arbitrary state the number of parameters and therefore measurements needed to fully specify the state grows exponentially with the system size. 
Restricted Boltzmann machines (RBM) are stochastic neural networks that can be used to learn and model properties of many-body systems \textbf{cite Troyer, Torlai, ...}. They have been shown to capture the properties of many-body systems and to be an effective tool for quantum state tomography.
This Python package provides a easy to use and open source RBM toolbox, which can be used to learn the wave function from experimental data, sample new data from the learned state and calculate a broad spectrum of observables directly from the learned state.


\section{Restricted Boltzmann Machine for Quantum State Tomography}


This tomography package allows to approximate a wave function $\psi( \boldsymbol{\sigma} )  = \langle \Psi \vert \boldsymbol{\sigma} \rangle$ in the reference basis $\{ \vert \boldsymbol{ \sigma} \rangle \}$ from experimental data. A so called Restricted Boltzmann Machine (RBM) can learn the distribution of the experimental data (e.g. qubit measurements $\vert \widehat{\boldsymbol{ \sigma}} \rangle = \vert \hat{ \sigma}_1~\hat{ \sigma}_2 \dots \rangle$) and allows after training to sample from it.

This file is a walk through for our code. For more theoretical background we refer to the following tutorials:
\textbf{ADD some refrences or Anna's File}

For the moment NNQuST can only handle binary inputs. Support for multinomial and continuous variable systems will come in the next version of NNQuST. Therefore measurements have to be translated into strings of $0$ and $1$ to be processed by the RBM.

\section{How to use NNQuST}

We will guide the user through short snippets of our code for different assumptions about the wave function.

\subsection{Training the NNQuST for a real positive wave function}

If all the measurements have been performed in the same basis and the experimenter assumes that the wave function is positive and real, the NNQuST package provides a more efficient tomography tool for this case. Since there are no unitaries that have to be applied on the measurements, this approach is highly parallelizable  and therefore faster.
First we load all the packages needed. \textbf{We might have to rename rbm to something like NNQuST} 

\begin{python}
from rbm import RBM #import NNQuST package
import numpy as np #generic math function
import torch #ML package
\end{python}

Prepare the input data. The input data needs to be in a numpy array or a torch tensor. E.g. for a spin system with 10 physical spins and measurements of every spin one input data point will be an array of the form \verb|np.array([1,0,1,1,0,1,0,0,0,1])|, with shape \verb|(10,)|. all the input data together needs to be an array of these arrays, which will have the shape \verb|(N,10)|. Where N is the number of data points in the training set.

For a dummy test we can define a training set:

\begin{python}
train_set = np.array([[1]*10]*1000) #define a dummy training set with the correct input shape
\end{python}

To run the RBM training we define the model parameters. The number of visible units is given by the length of one training sample. (In our case it is $10$). 
The number of hidden units can be varied and the success of the training depends on the complexity of the system and the ratio $\alpha$ between hidden and visible units. 
\textbf{refer to our scaling study and Giacomo's paper} 
The number of epochs strongly depends on the complexity of the system and also the training set size. 32 for the batch size is a good average value, which can be slightly changed without having big effects on the training success. The number of Gibbs sampling steps influences the speed of the training. The smaller we choose $k$, the faster is the training. High values on the other side give a better approximation for the model distribution \textbf{Put some referenece}. But it has been shown \textbf{Hinton}, that already for $k=1$ the RBM trains well.

\begin{python}
num_visible = 10 #number of visible units (has to be equal to input data dimension)
num_hidden = 10 #number of hidden units
epochs = 1000 #number of epochs for the training
batch_size = 32 #batch size for the training
k = 1 #Number of Gibbs sampling steps
learning_rate = 0.01 #Learning Rate
\end{python}



\begin{python}
rbm = RBM(num_visible=num_visible, num_hidden=num_hidden)

rbm.train(train_set, epochs, batch_size, k=k, lr=learning_rate)
\end{python}

This will already train the RBM.

\subsection{Training the NNQuST for a complex wave function}


\subsection{Saving, Loading the NNQuST}

To save and load the trained Tomography one can run the following code.

\begin{python}
location = 'some_folder/filename'
rbm.save(location) #save the weights and biases to some location
rbm.load(location) #load the weights and biases from some location
\end{python}

\section{What can I do with a trained RBM?}

\subsection{Sampling}

Since RBMs are generative models, one can sample new measurements from a trained RBM.

\begin{python}
num_samples = 100 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
rbm.sample(num_samples, k)
\end{python}

\subsection{Observables}

Observables like energy, magnetisation, etc. can be calculated directly on the sampled data from the RBM. To get a good average value one has to draw a big enough number $N$ of samples $\bm{s}$, such that the value converges. For a general observable $A$, we calculate,

\begin{equation}
\braket{A} = \frac{1}{N} \sum_{\bm{\sigma}} A(\bm{\sigma})
\end{equation}

\subsubsection{Example1: Magnetization of Ising chain}

The magnetization of an Ising chain can be calculated simply by averaging over all $\sigma_z$ measurements of the chain. Since the RBM only can sample in the $\sigma_z$ direction, we simply have to average over the sample $\bm{\sigma}$. The only additional step that has to be performed is the convergence from binary values $\{0,1  \}$ to actual spin measurements $\{-1 ,1  \}$.

\begin{python}
num_samples = 10000 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
samples = rbm.sample(num_samples, k)
samples = (samples-1/2)*2 # converge from binary to [-1, 1]
magnetization = 0 #initialize magnetization as 0
for s in samples: 
	magnetization += s.sum(0) # sum up every sample separately
magnetization /= num_samples # devide by number of samples
\end{python}

\subsubsection{Example2: Energy of Transverse Field Ising chain}

For the transverse field Ising model the Hamiltonian reads:

\begin{equation}
H = \sum_i \sigma^z_i \sigma^z_{i+1} + h \sum_i \sigma^x_{i}
\end{equation}

Calculating the energy for a sample $\ket{\bm{\sigma} }= \ket{\sigma_1 \dots  \sigma_n}$ is not that straight forward like for the magnetization, because the Hamiltonian is not state preserving, the $\sigma^x_i$ part of the Hamiltonian flips the spin at position $i$. So just calculating $\bra{\bm{\sigma} } H \ket{\bm{\sigma} }$ ignores the second part of the Hamiltonian completely. So what we have to do is the following:

\begin{align}
\ket{\psi} = \sum_{\bm{\sigma}} \psi(\bm{\sigma}) \ket{\bm{\sigma}} \\
\frac{\braket{\psi | H | \psi}}{\braket{\psi | \psi}} = \frac{\sum_{\bm{\sigma}, \bm{\sigma'}} \psi(\bm{\sigma}) \psi (\bm{\sigma}') H_{\bm{\sigma}, \bm{\sigma}'} \cdot \psi(\bm{\sigma}) / \psi(\bm{\sigma})}{ \sum_{\bm{\sigma}} | \psi(\bm{\sigma})|}  \\
=  \frac{1}{\sum_{\bm{\sigma}} | \psi({\bm{\sigma}} )|^2} \sum_{\bm{\sigma}} | \psi({\bm{\sigma}}) |^2 \left[ \sum_i \sigma_i \sigma_{i+1} + h \sum_i \frac{\psi(\sigma_1, \sigma_2, \dots , -\sigma_i, \sigma_{i+1} \dots)}{\psi (\bm{\sigma})} \right]\\
\approx \frac{1}{M} \sum_k^M \left[ \sum_i \sigma_i \sigma_{i+1} + h \sum_i \frac{\psi(\sigma_1, \sigma_2, \dots , -\sigma_i, \sigma_{i+1} \dots)}{\psi (\bm{\sigma})} \right]
\end{align}

Here we used that $H_{\bm{\sigma}, \bm{\sigma}'}  = \braket{\bm{\sigma} | H | \bm{\sigma}'} = \sum_i \delta_{\bm{\sigma}, \bm{\sigma}'} + h \sum_i \delta_{\sigma_1, \sigma_1'} \dots \delta_{\sigma_i, -\sigma_i'} \dots$ and that the partition function $Z$ cancels out if we divide the probabilities and the calculation is tractable also for big systems.
The observables.py package contains a few examples like this.

\begin{python}
from observables import TFIMChainEnergy
num_samples = 10000 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
h = 1.0 #define magnetic field strength
samples = rbm.sample(num_samples, k)
calc_energy = TFIMChainEnergy(h)
energy = calc_energy.apply(samples, rbm)
\end{python}

\subsection{Fidelity}

If one has access to the full quantum state $\ket{\phi} = \sum_{\bm{\sigma}} \phi(\bm{\sigma}) \ket{ \bm{\sigma}}$ and knows the coefficients $\phi(\bm{\sigma)}$ that are approximated by the RBM $\psi_{\Theta}(\bm{\sigma})$ the fidelity can be computed via $\sum_{\bm{\sigma}} \phi(\bm{\sigma)} \psi_{\Theta}(\bm{\sigma})$. This can be used to verify for example that experimental data actually comes from the state that should be prepared in an experiment. Because of the exponential growth of the wave function this is only feasible for small physical systems.

\begin{equation}
\psi_{\Theta}(\bm{\sigma}) = Z_{\Theta}^{-1/2} \sqrt{p_{\Theta} (\bm{\sigma})}
\end{equation}

with

\begin{align}
Z_{\Theta} &= \sum_{\bm{\sigma}} p_{\Theta}(\bm{\sigma}) \\
p_{\Theta} (\bm{\sigma}) &= \exp{ (- \varepsilon_{\Theta} (\bm{\sigma}))} \\
\varepsilon_{\Theta} (\bm{\sigma}) &= -\sum_{i,j} W_{i,j} h_i \sigma_j - \sum_j b_j \sigma_j - \sum_i c_i h_i
\end{align}


The effective energy $\varepsilon$ and the unnormalized probability $p_{\Theta}$ of some configuration 
\verb|torch.tensor([1,0,1,1,1,1,1,1,0,1])| can be accessed from the trained RBM via

\textbf{We need to rename free energy to effective energy in code!!!}

\begin{python}
some_configuration = torch.tensor([1,0,1,1,1,1,1,1,0,1], dtype = torch.double)
effective_energy = rbm.free_energy(some_configuration)
unnormalized_probability = rbm.unnormalized_probability(some_configuration) 
\end{python}

The partition function $Z_{\Theta}$ can be calculated by summing over all possible spin configurations, which is also the limiting factor for this approach for bigger systems.

\begin{python}
visble_space = rbm.generate_visble_space()
partition_fct = rbm.partition(visible_space)
\end{python}

To effectively calculate the fidelity with a theoretical state $\\ket{phi}$, one needs to load the coefficients $\phi(\bm{\sigma})$. In this example we create a random state with $2^N$ coefficients, where $N=10$ is the system size.

\begin{python}
N = 10 # Define system size
phi =  torch.abs(torch.randn(2**N)) # generate random vector (this would be the theoretical wave function)
phi /= phi.norm() # normalize the vector
effective_energy = rbm.free_energy(some_configuration)
unnormalized_probability = rbm.unnormalized_probability(some_configuration) 
\end{python}


\section{Algorithm for a reral positive wave function}
The training algorithm of the RBM has the following structure.
%=================================================================================
% RBM.train
 
\begin{algorithm}[H]
	 \caption{Training Algorithm of RBM. \textbf{RBM.train}() }
  \SetAlgoLined
  \For{batch in training set}{
  Load batch from training set, batch $=(\hat{ \boldsymbol{\sigma}}_1~\hat{ \boldsymbol{ \sigma}}_2 \dots)$\;
  compute the gradients from the batch $\Delta \bm{\Theta }  =$ ($\Delta W$, $\Delta b$, $\Delta c$)\\
  \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch)   \Comment*[r]{Algorithm 2}
  update weights and biases \\
$\bm{\Theta} \leftarrow \bm{\Theta } - \Delta \bm{\Theta }  $ \;
  }
 
\end{algorithm}

%=================================================================================
% RBM.compute gradient

The gradients are calculated according to the contrastive divergence algorithm, which allows us to approximate the probability distribution of the model with $k$ Gibbs sampling steps from the actual training data.

\begin{algorithm}[H]
	 \caption{Compute Gradient from Batch. \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch) }
  \SetAlgoLined
  Reset gradients $\Delta W$, $\Delta h_b$, $\Delta v_b$ $= 0$\;
  \For{$\hat{ \boldsymbol{ \sigma}}_i$ in batch}{
  sample $\bm{h}_0$, $\bm{v}_k$ and $\bm{p}_{h_k}$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
   \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$)   \Comment*[r]{Algorithm 3}
  calculate gradients\\
  $\Delta W += \bm{v}_0 \bm{h}_0^T - \bm{v}_k \bm{p}_{h_k}^T$ \\
  $\Delta c += \bm{h}_0 - \bm{p}_{h_k}$ \\
  $\Delta b += \bm{v}_0 - \bm{v}_k$ \;
  }
 $M = \vert batch \vert$ \;
 return $\Delta W / M$,  $\Delta c / M$, $\Delta b / M$
\end{algorithm}

%=================================================================================
% RBM.gibbs sampling

The Gibbs sampling is done $k$ times back and forth. The contrastive divergence algorithm already shows good results for $k=1$. But for better results this value can be increased.

\begin{algorithm}[H]
	 \caption{Gibbs sampling. \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$) }
  \SetAlgoLined
  calculate $\bm{p}_h$ and sample $\bm{h}_0$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
  \textbf{\lstinline{RBM.sample_h_given_v}}($\hat{ \boldsymbol{ \sigma}}$)   \Comment*[r]{Algorithm 4}
  $\bm{h} = \bm{h}_0$\;
  i = 0\;
  \While{i $\leq$ k}{
	calculate $\bm{p}(\bm{v}|\bm{h}_i)$ and sample $\bm{v}$ from $\bm{h}$\\
	\textbf{\lstinline{RBM.sample_v_given_h}}($\bm{h}$)   \Comment*[r]{Algorithm 5}
	calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ from $\bm{v}$\\
	\textbf{\lstinline{RBM.sample_h_given_v}}($\bm{v}$)   \Comment*[r]{Algorithm 4}
	$i +=1$
  }
  return $\bm{p}_{h_k} =$  $\bm{p}(\bm{h}|\bm{v})$, $\bm{v}_k = \bm{v}$ and $\bm{h}_0$\;
 
\end{algorithm}


%=================================================================================
% RBM.v given h and vice versa



\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{v}|\bm{h})$ and sample $\bm{v}$ \textbf{\lstinline{RBM.v_given_h}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{v} = 1|\bm{h}) = \sigma( W \bm{h} + v_b)$\;
  Bernoulli sample $\bm{v}$ from this probability\;
  return $\bm{v}$ and $\bm{p}(\bm{v} = 1|\bm{h})$\;
 
\end{algorithm}


\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ \textbf{\lstinline{RBM.h_given_v}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{h} = 1|\bm{v}) = \sigma(\bm{v}^T W  + h_b)$\;
  Bernoulli sample $\bm{h}$ from this probability\;
  return $\bm{h}$ and $\bm{p}(\bm{h} = 1|\bm{v})$\;
 
\end{algorithm}


\section{Conclusion}
You must include a conclusion.

\section*{Acknowledgements}
Acknowledgements should follow immediately after the conclusion.

% TODO: include author contributions
\paragraph{Author contributions}
This is optional. If desired, contributions should be succinctly described in a single short paragraph, using author initials.

% TODO: include funding information
\paragraph{Funding information}
Authors are required to provide funding information, including relevant agencies and grant numbers with linked author's initials. Correctly-provided data will be linked to funders listed in the \href{https://www.crossref.org/services/funder-registry/}{\sf Fundref registry}.


\begin{appendix}

\section{First appendix}
Add material which is better left outside the main text in a series of Appendices labeled by capital letters.

\section{About references}
Your references should start with the comma-separated author list (initials + last name), the publication title in italics, the journal reference with volume in bold, start page number, publication year in parenthesis, completed by the DOI link (linking must be implemented before publication). If using BiBTeX, please use the style files provided  on \url{https://scipost.org/submissions/author_guidelines}. If you are using our \LaTeX template, simply add
\begin{verbatim}
\bibliography{your_bibtex_file}
\end{verbatim}
at the end of your document. If you are not using our \LaTeX template, please still use our bibstyle as
\begin{verbatim}
\bibliographystyle{SciPost_bibstyle}
\end{verbatim}
in order to simplify the production of your paper.
\end{appendix}


% TODO:
% Provide your bibliography here. You have two options:

% FIRST OPTION - write your entries here directly, following the example below, including Author(s), Title, Journal Ref. with year in parentheses at the end, followed by the DOI number.
%\begin{thebibliography}{99}
%\bibitem{1931_Bethe_ZP_71} H. A. Bethe, {\it Zur Theorie der Metalle. i. Eigenwerte und Eigenfunktionen der linearen Atomkette}, Zeit. f{\"u}r Phys. {\bf 71}, 205 (1931), \doi{10.1007\%2FBF01341708}.
%\bibitem{arXiv:1108.2700} P. Ginsparg, {\it It was twenty years ago today... }, \url{http://arxiv.org/abs/1108.2700}.
%\end{thebibliography}

% SECOND OPTION:
% Use your bibtex library
% \bibliographystyle{SciPost_bibstyle} % Include this style file here only if you are not using our template
\bibliography{SciPost_Example_BiBTeX_File.bib}

\nolinenumbers

\end{document}
