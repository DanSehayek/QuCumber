\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{dsfont}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath, bm}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{braket}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{pythonhighlight}
\SetKwComment{Comment}{$\triangleright$\ }{}




\title{Theory}
\author{Patrick Huembeli}

\begin{document}

\section{What can I use this RBM Quantum State Tomography Library for?}

\section{Restricted Boltzmann Machine for Quantum State Tomography}


This tomography package allows to approximate a wave function $\psi( \boldsymbol{\sigma} )  = \langle \Psi \vert \boldsymbol{\sigma} \rangle$ in the reference basis $\{ \vert \boldsymbol{ \sigma} \rangle \}$ from experimental data. A so called Restricted Boltzmann Machine (RBM) can learn the distribution of the experimental data (e.g. qubit measurements $\vert \widehat{\boldsymbol{ \sigma}} \rangle = \vert \hat{ \sigma}_1~\hat{ \sigma}_2 \dots \rangle$) and allows after training to sample from it.

This file is a walk through for our code. For more theoretical background we refer to the following tutorials:
\textbf{ADD some refrences or Anna's File}

\section{How to use NNQuST}

We will guide the user through short snippets of our code. First we load all the packages needed. \textbf{We might have to rename rbm to something like NNQuST} 

\subsection{Training the NNQuST}

\begin{python}
from rbm import RBM #import NNQuST package
import numpy as np #generic math function
import torch #ML package
\end{python}

Prepare the input data. The input data needs to be in a numpy array or a torch tensor. E.g. for a spin system with 10 physical spins and measurements of every spin one input data point will be an array of the form \verb|np.array([1,0,1,1,0,1,0,0,0,1])|, with shape \verb|(10,)|. all the input data together needs to be an array of these arrays, which will have the shape \verb|(N,10)|. Where N is the number of data points in the training set.

For a dummy test we can define a training set:

\begin{python}
train_set = np.array([[1]*10]*1000) #define a dummy training set with the correct input shape
\end{python}

Define model parameters:

\begin{python}
num_visible = 10 #number of visible units (has to be equal to input data dimension)
num_hidden = 10 #number of hidden units
epochs = 1000 #number of epochs for the training
batch_size = 32 #batch size for the training
k = 1 #Number of Gibbs sampling steps
learning_rate = 0.01 #Learning Rate
\end{python}

\textbf{Get rid of SEED!!! it is already in ToDo list on Github}

\begin{python}
rbm = RBM(num_visible=num_visible, num_hidden=num_hidden, seed=seed)

rbm.train(train_set, epochs, batch_size, k=k, lr=learning_rate)
\end{python}

This will already train the RBM.

\subsection{Saving, Loading the NNQuST}

To save and load the trained Tomography one can run the following code.

\begin{python}
location = 'some_folder/filename'
rbm.save(location) #save the weights and biases to some location
rbm.load(location) #load the weights and biases from some location
\end{python}

\section{What can I do with a trained RBM?}

\subsection{Sampling}

\begin{python}
num_samples = 100 #number of samples drawn from the RBM
k = 100 #number of Gibbs steps for each sample
rbm.sample(num_samples, k)
\end{python}

\subsection{Observables}

Observables like energy, magnetisation, etc. can be calculated directly on the sampled data from the RBM

\subsection{Fidelity}

If one has access to the wave function $\psi(\bm{\sigma)}$ that is approximated by the RBM $\psi_{\Theta}(\bm{\sigma})$ the fidelity can be computed via $\sum_{\bm{\sigma}} \psi(\bm{\sigma)} \psi_{\Theta}(\bm{\sigma})$. This can be used to verify for example that experimental data actually comes from the state that should be prepared in an experiment. Because of the exponential growth of the wave function this is only feasible for small physical systems.


\section{Algorithm}
The training algorithm of the RBM has the following structure.
%=================================================================================
% RBM.train
 
\begin{algorithm}[H]
	 \caption{Training Algorithm of RBM. \textbf{RBM.train}() }
  \SetAlgoLined
  \For{batch in training set}{
  Load batch from training set, batch $=(\hat{ \boldsymbol{\sigma}}_1~\hat{ \boldsymbol{ \sigma}}_2 \dots)$\;
  compute the gradients from the batch $\Delta \bm{\Theta }  =$ ($\Delta W$, $\Delta b$, $\Delta c$)\\
  \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch)   \Comment*[r]{Algorithm 2}
  update weights and biases \\
$\bm{\Theta} \leftarrow \bm{\Theta } - \Delta \bm{\Theta }  $ \;
  }
 
\end{algorithm}

%=================================================================================
% RBM.compute gradient

The gradients are calculated according to the contrastive divergence algorithm, which allows us to approximate the probability distribution of the model with $k$ Gibbs sampling steps from the actual training data.

\begin{algorithm}[H]
	 \caption{Compute Gradient from Batch. \textbf{\lstinline{RBM.compute_batch_gradients}}(k, batch) }
  \SetAlgoLined
  Reset gradients $\Delta W$, $\Delta h_b$, $\Delta v_b$ $= 0$\;
  \For{$\hat{ \boldsymbol{ \sigma}}_i$ in batch}{
  sample $\bm{h}_0$, $\bm{v}_k$ and $\bm{p}_{h_k}$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
   \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$)   \Comment*[r]{Algorithm 3}
  calculate gradients\\
  $\Delta W += \bm{v}_0 \bm{h}_0^T - \bm{v}_k \bm{p}_{h_k}^T$ \\
  $\Delta c += \bm{h}_0 - \bm{p}_{h_k}$ \\
  $\Delta b += \bm{v}_0 - \bm{v}_k$ \;
  }
 $M = \vert batch \vert$ \;
 return $\Delta W / M$,  $\Delta c / M$, $\Delta b / M$
\end{algorithm}

%=================================================================================
% RBM.gibbs sampling

The Gibbs sampling is done $k$ times back and forth. The contrastive divergence algorithm already shows good results for $k=1$. But for better results this value can be increased.

\begin{algorithm}[H]
	 \caption{Gibbs sampling. \textbf{\lstinline{RBM.gibbs_sampling}}(k, $\hat{ \boldsymbol{ \sigma}}_i$) }
  \SetAlgoLined
  calculate $\bm{p}_h$ and sample $\bm{h}_0$ from $\hat{ \boldsymbol{ \sigma}}_i$\\
  \textbf{\lstinline{RBM.sample_h_given_v}}($\hat{ \boldsymbol{ \sigma}}$)   \Comment*[r]{Algorithm 4}
  $\bm{h} = \bm{h}_0$\;
  i = 0\;
  \While{i $\leq$ k}{
	calculate $\bm{p}(\bm{v}|\bm{h}_i)$ and sample $\bm{v}$ from $\bm{h}$\\
	\textbf{\lstinline{RBM.sample_v_given_h}}($\bm{h}$)   \Comment*[r]{Algorithm 5}
	calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ from $\bm{v}$\\
	\textbf{\lstinline{RBM.sample_h_given_v}}($\bm{v}$)   \Comment*[r]{Algorithm 4}
	$i +=1$
  }
  return $\bm{p}_{h_k} =$  $\bm{p}(\bm{h}|\bm{v})$, $\bm{v}_k = \bm{v}$ and $\bm{h}_0$\;
 
\end{algorithm}


%=================================================================================
% RBM.v given h and vice versa



\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{v}|\bm{h})$ and sample $\bm{v}$ \textbf{\lstinline{RBM.v_given_h}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{v} = 1|\bm{h}) = \sigma( W \bm{h} + v_b)$\;
  Bernoulli sample $\bm{v}$ from this probability\;
  return $\bm{v}$ and $\bm{p}(\bm{v} = 1|\bm{h})$\;
 
\end{algorithm}


\begin{algorithm}[H]
	 \caption{calculate $\bm{p}(\bm{h}|\bm{v})$ and sample $\bm{h}$ \textbf{\lstinline{RBM.h_given_v}}() }
  \SetAlgoLined
  calculate probability $\bm{p}(\bm{h} = 1|\bm{v}) = \sigma(\bm{v}^T W  + h_b)$\;
  Bernoulli sample $\bm{h}$ from this probability\;
  return $\bm{h}$ and $\bm{p}(\bm{h} = 1|\bm{v})$\;
 
\end{algorithm}



\end{document}\grid
