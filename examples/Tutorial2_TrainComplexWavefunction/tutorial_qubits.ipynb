{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of a complex wavefunction\n",
    "\n",
    "In this tutorial, a walkthrough of how to reconstruct a **complex** wavefunction via training a *Restricted Boltzmann Machine* (RBM), the neural network behind qucumber, will be presented.\n",
    "\n",
    "\n",
    "## The wavefunction to be reconstructed\n",
    "The simple wavefunction below describing two qubits (coefficients stored in *qubits_psi.txt*) will be reconstructed.\n",
    "\n",
    "\\begin{equation}\n",
    "            \\vert\\psi \\rangle = \\alpha \\vert00\\rangle + \\beta \\vert 01\\rangle + \\gamma \\vert10\\rangle + \\delta \\vert11\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "where the exact values of $\\alpha, \\beta, \\gamma$ and $\\delta$ used for this tutorial are \n",
    "\n",
    "\\begin{align}\n",
    "\\alpha & = 0.2861  + 0.0539 i \\\\\n",
    "\\beta &  = 0.3687 - 0.3023 i \\\\\n",
    "\\gamma & = -0.1672 - 0.3529 i \\\\\n",
    "\\delta & = -0.5659 - 0.4639 i.\n",
    "\\end{align}\n",
    "\n",
    "The example dataset, *qubits_train.txt*, comprises of 500 $\\sigma$ measurements made in various bases (X, Y and Z). A corresponding file containing the bases for each data point in *qubits_train.txt*, *qubits_train_bases.txt*, is also required. As per convention, spins are represented in binary notation with zero and one denoting spin-down and spin-up, respectively.\n",
    "\n",
    "## Using qucumber to reconstruct the wavefunction\n",
    "\n",
    "### Imports\n",
    "To begin the tutorial, first import the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qucumber.nn_states import ComplexWavefunction\n",
    "\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.unitaries as unitaries\n",
    "import qucumber.utils.cplx as cplx\n",
    "\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python class *ComplexWavefunction* contains generic properties of a RBM meant to reconstruct a complex wavefunction, the most notable one being the gradient function required for stochastic gradient descent.\n",
    "\n",
    "To instantiate a *ComplexWavefunction* object, one needs to specify the number of visible and hidden units in the RBM. The number of visible units, *num_visible*, is given by the size of the physical system, i.e. the number of spins or qubits (2 in this case), while the number of hidden units, *num_hidden*, can be varied to change the expressiveness of the neural network.\n",
    "\n",
    "**Note:** The optimal *num_hidden* : *num_visible* ratio will depend on the system. For the two-qubit wavefunction described above, good results are yielded when this ratio is 1.\n",
    "\n",
    "On top of needing the number of visible and hidden units, a *ComplexWavefunction* object requires the user to input a dictionary containing the unitary operators (2x2) that will be used to rotate the qubits in and out of the computational basis, Z, during the training process. The *unitaries* utility will take care of creating this dictionary.\n",
    "\n",
    "The *MetricEvaluator* class and *training_statistics* utility are built-in amenities that will allow the user to evaluate the training in real time. \n",
    "\n",
    "Lastly, the *cplx* utility allows qucumber to be able to handle complex numbers. Currently, Pytorch does not support complex numbers.\n",
    "\n",
    "\n",
    "### Training\n",
    "To evaluate the training in real time, the fidelity between the true wavefunction of the system and the wavefunction that qucumber reconstructs, $\\vert\\langle\\psi\\vert\\psi_{RBM}\\rangle\\vert^2$, will be calculated along with the Kullback-Leibler (KL) divergence (the RBM's cost function). First, the training data and the true wavefunction of this system need to be loaded using the *data* utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path       = \"qubits_train.txt\"\n",
    "train_bases_path = \"qubits_train_bases.txt\"\n",
    "psi_path         = \"qubits_psi.txt\"\n",
    "bases_path       = \"qubits_bases.txt\"\n",
    "\n",
    "train_samples, true_psi, train_bases, bases = data.load_data(train_path, psi_path, train_bases_path, bases_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file *qubits_bases.txt* contains every unique basis in the *qubits_train_bases.txt* file. Calculation of the full KL divergence in every basis requires the user to specify each unique basis.\n",
    "\n",
    "As previouosly mentioned, a *ComplexWavefunction* object requires a dictionary that contains the unitariy operators that will be used to rotate the qubits in and out of the computational basis, Z, during the training process. In the case of the provided dataset, the unitaries required are the well-known $H$, and $K$ gates. The dictionary needed can be created with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary_dict = unitaries.create_dict()\n",
    "#unitary_dict = unitaries.create_dict(unitary_name=torch.tensor([[real part], \n",
    "#                                                                 [imaginary part]], \n",
    "#                                                                 dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wishes to add their own unitary operators from their experiment to *unitary_dict*, uncomment the block above. When *unitaries.create_dict()* is called, it will contain the identity and the $H$ and $K$ gates by default with the keys \"Z\", \"X\" and \"Y\", respectively.\n",
    "\n",
    "The number of visible units in the RBM is equal to the number of qubits. The number of hidden units will also be taken to be the number of visible units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = train_samples.shape[-1]\n",
    "nh = nv\n",
    "\n",
    "nn_state = ComplexWavefunction(num_visible=nv, num_hidden=nh, unitary_dict=unitary_dict, gpu=False)\n",
    "#nn_state = ComplexWavefunction(num_visible=nv, num_hidden=nh, unitary_dict=unitary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, qucumber will attempt to run on a GPU if one is available (if one is not available, qucumber will default to CPU). If one wishes to run qucumber on a CPU, add the flag \"gpu = False\" in the *ComplexWavefunction* object instantiation. Uncomment the line above to run this tutorial on a GPU.\n",
    "\n",
    "Now the hyperparameters of the training process can be specified. \n",
    "\n",
    "1. **epochs**: the total number of training cycles that will be performed (default = 100)\n",
    "2. **pos_batch_size**: the number of data points used in the positive phase of the gradient (default = 100)\n",
    "3. **neg_batch_size**: the number of data points used in the negative phase of the gradient (default = *pos_batch_size*)\n",
    "4. **k**: the number of contrastive divergence steps (default = 1)\n",
    "5. **lr**: the learning rate (default = 0.001)\n",
    "\n",
    "    **Note:** For more information on the hyperparameters above, it is strongly encouraged that the user to read through the brief, but thorough theory document on RBMs. One does not have to specify these hyperparameters, as their default values will be used without the user overwriting them. It is recommended to keep with the default values until the user has a stronger grasp on what these hyperparameters mean. The quality and the computational efficiency of the training will highly depend on the choice of hyperparameters. As such, playing around with the hyperparameters is almost always necessary. \n",
    "    \n",
    "The two-qubit example in this tutorial should be extremely easy to train, regardless of the choice of hyperparameters. However, the hyperparameters below will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "pbs    = 50 # pos_batch_size\n",
    "nbs    = 10 # neg_batch_size\n",
    "lr     = 0.1\n",
    "k      = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the training in real time, the *MetricEvaluator* will be called to calculate the training evaluators every 10 epochs. The *MetricEvaluator* requires the following arguments.\n",
    "\n",
    "1. **log_every**: the frequency of the training evaluators being calculated is controlled by the *log_every* argument (e.g. *log_every* = 200 means that the *MetricEvaluator* will update the user every 200 epochs)\n",
    "2. A dictionary of functions you would like to reference to evaluate the training (arguments required for these functions are keyword arguments placed after the dictionary)\n",
    "\n",
    "The following additional arguments are needed to calculate the fidelity and KL divergence in the *training_statistics* utility.\n",
    "\n",
    "- **target_psi** (the true wavefunction of the system)\n",
    "- **space** (the hilbert space of the system)\n",
    "\n",
    "The training evaluators can be printed out via the *verbose=True* statement.\n",
    "\n",
    "Although the fidelity and KL divergence are excellent training evaluators, they are not practical to calculate in most cases; the user may not have access to the target wavefunction of the system, nor may generating the hilbert space of the system be computationally feasible. However, evaluating the training in real time is extremely convenient. \n",
    "\n",
    "Any custom function that the user would like to use to evaluate the training can be given to the *MetricEvaluator*, thus avoiding having to calculate fidelity and/or KL divergence. As an example, functions that calculate the the norm of each of the reconstructed wavefunction's coefficients are presented. Any custom function given to *MetricEvaluator* must take the neural-network state (in this case, the *ComplexWavefunction* object) and keyword arguments. Although the given example requires the hilbert space to be computed, the scope of the *MetricEvaluator*'s ability to be able to handle any function should still be evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(nn_state, space, **kwargs):\n",
    "    rbm_psi       = nn_state.psi(space)\n",
    "    normalization = nn_state.compute_normalization(space).sqrt_()\n",
    "    alpha_        = cplx.norm(torch.tensor([rbm_psi[0][0], rbm_psi[1][0]], \n",
    "                                           device = nn_state.device)/normalization)\n",
    "    \n",
    "    return alpha_\n",
    "\n",
    "def beta(nn_state, space, **kwargs):\n",
    "    rbm_psi       = nn_state.psi(space)\n",
    "    normalization = nn_state.compute_normalization(space).sqrt_()\n",
    "    beta_        = cplx.norm(torch.tensor([rbm_psi[0][1], rbm_psi[1][1]], \n",
    "                                          device = nn_state.device)/normalization)\n",
    "    \n",
    "    return beta_\n",
    "\n",
    "def gamma(nn_state, space, **kwargs):\n",
    "    rbm_psi       = nn_state.psi(space)\n",
    "    normalization = nn_state.compute_normalization(space).sqrt_()\n",
    "    gamma_        = cplx.norm(torch.tensor([rbm_psi[0][2], rbm_psi[1][2]], \n",
    "                                           device = nn_state.device)/normalization)\n",
    "    \n",
    "    return gamma_\n",
    "\n",
    "def delta(nn_state, space, **kwargs):\n",
    "    rbm_psi       = nn_state.psi(space)\n",
    "    normalization = nn_state.compute_normalization(space).sqrt_()\n",
    "    delta_        = cplx.norm(torch.tensor([rbm_psi[0][3], rbm_psi[1][3]], \n",
    "                                           device = nn_state.device)/normalization)\n",
    "    \n",
    "    return delta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the hilbert space of the system must be generated for the fidelity and KL divergence and the dictionary of functions the user would like to compute every \"*log_every*\" epochs must be given to the *MetricEvaluator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every = 10\n",
    "space = nn_state.generate_hilbert_space(nv)\n",
    "\n",
    "callbacks = [\n",
    "    MetricEvaluator(log_every, {\"Fidelity\": ts.fidelity, \"KL\": ts.KL, \n",
    "                                \"normα\": alpha, \"normβ\": beta, \n",
    "                                \"normγ\": gamma, \"normδ\": delta}, \n",
    "                    target_psi=true_psi, bases=bases, verbose=True, space=space)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training can begin. The *ComplexWavefunction* object has a property called *fit* which takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\tFidelity = 0.680318\tKL = 0.192934\tnormα = 0.196213\tnormβ = 0.343945\tnormγ = 0.417829\tnormδ = 0.817693\n",
      "Epoch: 20\tFidelity = 0.918409\tKL = 0.040900\tnormα = 0.292956\tnormβ = 0.465326\tnormγ = 0.417334\tnormδ = 0.723520\n",
      "Epoch: 30\tFidelity = 0.963046\tKL = 0.024027\tnormα = 0.290475\tnormβ = 0.475844\tnormγ = 0.406877\tnormδ = 0.723635\n",
      "Epoch: 40\tFidelity = 0.977946\tKL = 0.019233\tnormα = 0.292512\tnormβ = 0.493205\tnormγ = 0.393803\tnormδ = 0.718405\n",
      "Epoch: 50\tFidelity = 0.984567\tKL = 0.014083\tnormα = 0.286016\tnormβ = 0.478454\tnormγ = 0.403332\tnormδ = 0.725672\n"
     ]
    }
   ],
   "source": [
    "nn_state.fit(train_samples, epochs=epochs, pos_batch_size=pbs, neg_batch_size=nbs, lr=lr, k=k, input_bases=train_bases,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these training evaluators can be accessed after the training has completed, as well. The code below shows this, along with plots of each training evaluator versus the training cycle number (epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/callbacks/metric_evaluator.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, metric)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/callbacks/metric_evaluator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'normalpha'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7710306e10e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfidelities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFidelity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mKLs\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoeffs\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mepoch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/callbacks/metric_evaluator.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, metric)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fidelities = callbacks[0].Fidelity\n",
    "KLs        = callbacks[0].KL\n",
    "coeffs     = callbacks[0].normalpha\n",
    "epoch      = np.arange(log_every, epochs+1, log_every)\n",
    "\n",
    "plt.figure(1)\n",
    "ax1 = plt.axes()\n",
    "ax1.grid()\n",
    "ax1.set_xlim(log_every, epochs)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Fidelity\")\n",
    "ax1.plot(epoch, fidelities, color='r')\n",
    "\n",
    "plt.figure(2)\n",
    "ax2 = plt.axes()\n",
    "ax2.grid()\n",
    "ax2.set_xlim(log_every, epochs)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"KL Divergence\")\n",
    "ax2.plot(epoch, KLs, color='r')\n",
    "\n",
    "plt.figure(3)\n",
    "ax3 = plt.axes()\n",
    "ax3.grid()\n",
    "ax3.set_xlim(log_every, epochs)\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.set_ylabel(r\"$\\vert\\alpha\\vert$\")\n",
    "ax3.plot(epoch, coeffs, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that one could have just ran *nn_state.fit(train_samples)* and just used the default hyperparameters and no training evaluators.\n",
    "\n",
    "At the end of the training process, the network parameters (the weights, visible biases and hidden biases) are stored in the *ComplexWavefunction* object. One can save them to a pickle file, which will be called *saved_params.pt*, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save(\"saved_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the weights, visible biases and hidden biases as torch tensors with the following keys: \"weights\", \"visible_bias\", \"hidden_bias\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
