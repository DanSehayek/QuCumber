{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Reconstruction of a positive wavefunction\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The following imports are needed to run this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rbm_tutorial import RBM_Module, BinomialRBM\n",
    "import torch\n",
    "#from observables_tutorial import TFIMChainEnergy, TFIMChainMagnetization\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../qucumber/')\n",
    "from positive_wavefunction import PositiveWavefunction\n",
    "from quantum_reconstruction import QuantumReconstruction\n",
    "sys.path.append('../qucumber/utils/')\n",
    "import utils.training_statistics as ts\n",
    "sys.path.append('../qucumber/utils/ed/')\n",
    "from hamiltonians import *\n",
    "from data_generator import *\n",
    "sys.path.append('observables/')\n",
    "import quantum_ising_chain as TFIM\n",
    "#import importlib.util\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*rbm_tutorial.py* contains the child class **BinomialRBM** that inherits properties and functions from the parent class **RBM_Module**. \n",
    " \n",
    "PyTorch is used as a replacement for doing some algebra that would normally be done with numpy. PyTorch also allows one to take advantage of GPU acceleration among many other things. If a GPU card is not available, the tutorial will run on a CPU by default.\n",
    "\n",
    "*observables_tutorial.py* is a class that will allow us to calculate physical properties like the energy and magnetization from samples generated by the trained RBM.\n",
    "\n",
    "## Training\n",
    "\n",
    "Let's beging with training the RBM on a positive wavefunction. We consider the quantum Ising model with Hamiltonian $H=-J\\sum_{\\langle i j \\rangle} S^z_i S^z_j - h \\sum_i S^x_i$\n",
    "at its quantum critical point $h/J=1$.  The training data has been generated and is contained in the file *tfim1d_N10_train_samples.txt*.  It contains 10,000 measurements of the $S^z$ states of 10 qubits, represented as zeros or ones.\n",
    "\n",
    "To evaluate how well the RBM is training, we compute the fidelity, $|\\langle \\psi|\\psi_{\\rm RBM} \\rangle|^2$, between the true wavefunction of the system and the wavefunction the RBM reconstructs. First, we need to load our training data and the true wavefunction of this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'tfim1d'   #Model name\n",
    "N = 4             #Number of spins\n",
    "Nsamples = 1000    #Number of measurements\n",
    "hx=1.0             #Magnetic field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hamiltonian=TransverseFieldIsing(N,hx)    #Build quantum Hamiltonian\n",
    "(en,psi) = np.linalg.eigh(Hamiltonian)        #Full diagonalization\n",
    "target_psi = psi[:,0]   #Ground state wavefunction\n",
    "gs_energy = en[0]       #Ground state energy                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = GenerateDataSet(N,Nsamples,target_psi)['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following arguments are required to construct a **BinomialRBM** object:\n",
    "\n",
    "1. **The number of visible units, *num_visible***. This is 10 for the case of our dataset.\n",
    "2. **The number of hidden units in the hidden layer of the RBM, *num_hidden***. This number is set to the number of visible units by default (10 in the case of our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = N\n",
    "nh  = nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **BinomialRBM** object has a function called *fit* that performs the training. *fit* takes the following arguments:\n",
    "\n",
    "1. **train_set**: needed for selecting mini batches of the data\n",
    "2. **true_psi**: only needed here to compute the fidelity\n",
    "3. **epochs**: the number of epochs, i.e. training cycles that will be performed; 1000 should be fine\n",
    "4. **batch_size**: the number of data points that each mini batch will contain; we'll go with 100\n",
    "5. **k**: the number of contrastive divergence steps; k=1 seems to be good enough in most cases\n",
    "6. **lr**: the learning rate; we will use a learning rate of 0.01 here\n",
    "7. **log_every**: how often you would like the program to update you during the training; we choose 50 - that is, every 50 epochs the program will print out the fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 1000\n",
    "num_chains = 100\n",
    "batch_size = 100\n",
    "k          = 10\n",
    "lr         = 0.05\n",
    "log_every  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state = PositiveWavefunction(num_visible=nv,num_hidden=nh, seed=1234)\n",
    "qr = QuantumReconstruction(nn_state)\n",
    "input_psi = torch.tensor(np.asarray([psi[:,0],np.zeros((psi.shape[0]))]),dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = ts.TrainingStatistics(train_samples.shape[-1],log_every)\n",
    "train_stats.load(psi[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100 \tFidelity = 0.930662\n",
      "Epoch = 200 \tFidelity = 0.975502\n",
      "Epoch = 300 \tFidelity = 0.980206\n",
      "Epoch = 400 \tFidelity = 0.982410\n",
      "Epoch = 500 \tFidelity = 0.984257\n",
      "Epoch = 600 \tFidelity = 0.986062\n",
      "Epoch = 700 \tFidelity = 0.988147\n",
      "Epoch = 800 \tFidelity = 0.989638\n",
      "Epoch = 900 \tFidelity = 0.991954\n",
      "Epoch = 1000 \tFidelity = 0.993766\n"
     ]
    }
   ],
   "source": [
    "qr.fit(train_samples, epochs, batch_size, num_chains, k,lr, progbar=False,observer = train_stats)#target_psi=input_psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training \n",
    "\n",
    "After training your RBM, the *fit* function will have saved your trained weights and biases. Now, we can generate samples from our trained RBM and calculate physical observables. Let's calculate the energy and the magnetization of newly generated samples.\n",
    "\n",
    "**TFIMChainEnergy** and **TFIMChainMagnetization** objects (from *observables_tutorial.py*) have a sampler function called *sample* built into them. They will generate samples from the RBM distribution that was just learned in the training procedure, compute the observables and plot the computed values as a function of the Gibbs step.\n",
    "\n",
    "All that needs to be done is to feed in the following arguments into the objects' *sample* functions:\n",
    "\n",
    "1. **sampler**: the RBM object that we've trained\n",
    "2. **num_samples**: the number of samples we wish to generate\n",
    "3. **k**: the number of Gibbs steps that will be used to generate samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1813, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n_measurements= 10000\n",
    "tfim = TFIM.TransverseFieldIsingChain(hx,n_measurements)\n",
    "Energy = tfim.Energy(nn_state,n_eq=1000)\n",
    "print(Energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rbm_mag    = TFIMChainMagnetization()\n",
    "#magnetizations = rbm_mag.sample(sampler=rbm_real, num_samples=3000, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a brief transient period in each observable, before the state of the machine \"warms up\" to equilibrium.  After that, the values fluctuate around the mean.  The exact value for the energy is -1.2785, and for the magnetization is 0.7072.\n",
    "\n",
    "And there you have it! For more information on using QuCumber on your machine, please refer to [here](../tutorial.rst). If you are interested in learning about training a **ComplexRBM** object, please click next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
