{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN DEVELOPMENT\n",
    "\n",
    "# Part 2: Training an RBM *with* a phase\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The following imports are needed to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from rbm_tutorial import RBM_Module, BinomialRBM\\nimport torch\\n#from observables_tutorial import TFIMChainEnergy, TFIMChainMagnetization\\nimport numpy as np\\nimport csv\\n%matplotlib inline\\nimport sys\\nsys.path.append('../../qucumber/')\\nfrom complex_wavefunction import ComplexWavefunction\\nfrom quantum_reconstruction import QuantumReconstruction\\nsys.path.append('../../qucumber/utils/')\\nimport unitaries\\nimport utils.training_statistics as ts\\nimport pickle\\n#import importlib.util\\n#%load_ext autoreload\\n#%autoreload 2\\n#%matplotlib notebook\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from qucumber.binary_rbm import BinaryRBM\n",
    "from qucumber.quantum_reconstruction import QuantumReconstruction\n",
    "from qucumber.complex_wavefunction import ComplexWavefunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.data as data              # for importing data\n",
    "import qucumber.utils.cplx as cplx              # for complex algebra in torch\n",
    "import qucumber.utils.unitaries as unitaries    # for importing unitary operators / gates\\\n",
    "import qucumber.utils.training_statistics as ts\n",
    "\n",
    "'''\n",
    "#from rbm_tutorial import RBM_Module, BinomialRBM\n",
    "import torch\n",
    "#from observables_tutorial import TFIMChainEnergy, TFIMChainMagnetization\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../qucumber/')\n",
    "from complex_wavefunction import ComplexWavefunction\n",
    "from quantum_reconstruction import QuantumReconstruction\n",
    "sys.path.append('../../qucumber/utils/')\n",
    "import unitaries\n",
    "import utils.training_statistics as ts\n",
    "import pickle\n",
    "#import importlib.util\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib notebook\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _BinaryRBM_ class in *binary_rbm.py* contains the generic properties of an RBM with a binary visible and hidden layer (e.g. it's effective energy and sampling the hidden and visible layers). \n",
    "\n",
    "The actual quantum wavefunction reconstruction occurs in the _QuantumReconstruction_ class in *quantum_reconstruction.py*. A _QuantumReconstruction_ object is initialized with a neural network state (in this case, a *ComplexWavefunction* object).\n",
    "\n",
    "*MetricEvaluator* in *callbacks.py* contains functions that allow the user to evaluate the quality of the training (i.e. based on the fidelity or KL divergence).\n",
    "\n",
    "## Training\n",
    "\n",
    "Let's go through training a complex wavefunction. To evaluate how the RBM is training, we will compute the full KL divergence and the fidelity between the true wavefunction of the system and the wavefunction the RBM reconstructs. We first need to load our training data and the true wavefunction of this system. However, we also need the corresponding file that contains all of the measurements that each site is in. The dummy dataset we will train our RBM on is a two qubit system who's wavefunction is $\\psi =\\left.\\frac{1}{2}\\right\\vert+,+\\rangle - \\left.\\frac{1}{2}\\right\\vert+,-\\rangle + \\left.\\frac{i}{2}\\right\\vert-,+\\rangle - \\left.\\frac{i}{2}\\right\\vert-,-\\rangle$, where $+$ and $-$ represent spin-up and spin-down, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_path = 'qubits_train_samples.txt'\n",
    "train_bases_path   = 'qubits_train_bases.txt'\n",
    "bases_path         = 'qubits_bases.txt'\n",
    "psi_path           = 'qubits_psi.txt'\n",
    "\n",
    "train_samples,target_psi,train_bases,bases = data.load_data(train_samples_path, \n",
    "                                                            psi_path, \n",
    "                                                            train_bases_path, \n",
    "                                                            bases_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following arguments are required to construct a **ComplexWavefunction** neural network state:\n",
    "\n",
    "1. **A dictionary of unitary operators**. This will contain the (2 x 2) unitary matrices / gates.\n",
    "2. **The number of visible units**. This is 2 for the case of our dataset.\n",
    "3. **The number of hidden units in the hidden layer of the RBM**. This number is set to the number of visible units by default (10 in the case of our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary_dict = unitaries.create_dict()\n",
    "'''If you would like to add your own quantum gates from your experiment to \n",
    "   \"unitary_dict\", do:\n",
    "   unitary_dict = unitaries.create_dict(name='your_name', \n",
    "                                        unitary=torch.tensor([[real part], \n",
    "                                                              [imaginary part]], \n",
    "                                                             dtype=torch.double)\n",
    "                                                             \n",
    "   For example: \n",
    "   unitaries = unitary_library.create_dict(name='qucumber', \n",
    "                                           unitary=torch.tensor([ [[1.,0.],[0.,1.]] \n",
    "                                                                  [[0.,0.],[0.,0.]] ], \n",
    "                                                                dtype=torch.double))\n",
    "                                                                                             \n",
    "   By default, unitary_library.create_dict() contains the idenity matrix and the \n",
    "   hadamard and K gates with keys Z, X and Y, respectively.\n",
    "'''\n",
    "\n",
    "nv = train_samples.shape[-1]\n",
    "nh = nv\n",
    "\n",
    "nn_state = ComplexWavefunction(unitary_dict, num_visible=nv, num_hidden=nh, seed=1206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the parameters of the training process:\n",
    "\n",
    "1. **epochs**: the number of epochs, i.e. training cycles that will be performed; 1000 should be fine\n",
    "2. **batch_size**: the number of data points used in the positive phase of the gradient; we'll go with 100\n",
    "3. **num_chains**: the number of data points used in the negative phase of the gradient. Keeping this larger than the *batch_size* is preferred; we'll go with 200\n",
    "4. **CD**: the number of contrastive divergence steps; CD=1 seems to be good enough in most cases\n",
    "5. **lr**: the learning rate; we will use a learning rate of 0.01 here\n",
    "6. **log_every**: how often you would like the program to update you during the training; we choose 50 - that is, every 50 epochs the program will print out the fidelity. This parameter is required in the MetricEvaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs     = 50\n",
    "num_chains = 10\n",
    "batch_size = 5\n",
    "CD          = 5\n",
    "lr         = 0.1\n",
    "log_every  = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize the parameters of the *ComplexWavefunction* and the *MetricEvaluator*, we can now begin training. Our *QuantumReconstruction* object, *qr* (see below), contains a function called *fit* that executes the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaacdevlugt/anaconda3/envs/testing/lib/python3.6/site-packages/torch-0.4.1-py3.6-linux-x86_64.egg/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1\tFidelity = 0.669944\tKL = 0.279684\t\n",
      "Epoch = 2\tFidelity = 0.682818\tKL = 0.264968\t\n",
      "Epoch = 3\tFidelity = 0.705441\tKL = 0.249516\t\n",
      "Epoch = 4\tFidelity = 0.729787\tKL = 0.226180\t\n",
      "Epoch = 5\tFidelity = 0.747837\tKL = 0.218815\t\n",
      "Epoch = 6\tFidelity = 0.787030\tKL = 0.189679\t\n",
      "Epoch = 7\tFidelity = 0.801210\tKL = 0.179870\t\n",
      "Epoch = 8\tFidelity = 0.815647\tKL = 0.174167\t\n",
      "Epoch = 9\tFidelity = 0.848333\tKL = 0.147757\t\n",
      "Epoch = 10\tFidelity = 0.889060\tKL = 0.109471\t\n",
      "Epoch = 11\tFidelity = 0.915754\tKL = 0.083433\t\n",
      "Epoch = 12\tFidelity = 0.936109\tKL = 0.061922\t\n",
      "Epoch = 13\tFidelity = 0.943627\tKL = 0.057871\t\n",
      "Epoch = 14\tFidelity = 0.968959\tKL = 0.029827\t\n",
      "Epoch = 15\tFidelity = 0.978589\tKL = 0.020454\t\n",
      "Epoch = 16\tFidelity = 0.984242\tKL = 0.014953\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cb5e8cfe4381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m qr.fit(train_samples, epochs, batch_size, num_chains, CD,\n\u001b[1;32m      9\u001b[0m        \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_bases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_bases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m        z_samples = z_samples)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.1.2-py3.6.egg/qucumber/quantum_reconstruction.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input_samples, epochs, pos_batch_size, neg_batch_size, k, lr, observer, input_bases, z_samples, progbar, callbacks)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mbatch_bases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_batches_bases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mall_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_bases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear any cached gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.1.2-py3.6.egg/qucumber/quantum_reconstruction.py\u001b[0m in \u001b[0;36mcompute_batch_gradients\u001b[0;34m(self, k_cd, samples_batch, bases_batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# Positive phase: learning signal driven by the data (and bases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mdata_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbases_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mgrad_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Accumulate amplitude RBM gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mgrad_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Accumulate phase RBM gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.1.2-py3.6.egg/qucumber/complex_wavefunction.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, basis, sample)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# Gradient on the current configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mgrad_vp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbm_am\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_energy_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbm_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_energy_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;31m# NN state rotated in this bases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.1.2-py3.6.egg/qucumber/binary_rbm.py\u001b[0m in \u001b[0;36meffective_energy_gradient\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mc_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ij->j\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparameters_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mW_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_grad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_v_given_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/torch-0.4.1-py3.6-linux-x86_64.egg/torch/nn/utils/convert_parameters.py\u001b[0m in \u001b[0;36mparameters_to_vector\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_state.space = nn_state.generate_Hilbert_space(nv) # generate the entire visible space of the system.\n",
    "callbacks      = [MetricEvaluator(log_every,{'Fidelity':ts.fidelity,'KL':ts.KL},target_psi=target_psi.to(nn_state.device),bases=bases)]\n",
    "z_samples      = data.extract_refbasis_samples(train_samples,train_bases).to(nn_state.device) # required for the negative phase of the\n",
    "                                                                          # gradient of the effective energy\n",
    "print (z_samples.device)\n",
    "qr = QuantumReconstruction(nn_state)\n",
    "\n",
    "qr.fit(train_samples, epochs, batch_size, num_chains, CD,\n",
    "       lr, input_bases=train_bases, progbar=False, callbacks = callbacks,\n",
    "       z_samples = z_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Training \n",
    "\n",
    "After training your RBM, the *fit* function will have stored your trained weights and biases for the amplitude and the phase. Now, you have the option to generate new data from the trained RBM. The *rbm_real* object has a *sample* function that takes the following arguments.\n",
    "\n",
    "1. The number of samples you wish to generate, *num_samples*.\n",
    "2. The number of contrastive divergence steps performed to generate the samples, *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2000\n",
    "CD          = 200\n",
    "\n",
    "nn_state.sample(CD)\n",
    "samples = nn_state.visible_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the RBM parameters and the newly generated samples using the *save* function within the ComplexWavefunction object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save('saved_parameters.pkl', metadata={'Samples':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
