{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-qubit complex wavefunction\n",
    "\n",
    "In this tutorial, we use QuCumber reconstruct the most likely wavefunction from measurements of tow qubits. In constrast with the TFIM tutorial, this wavefunction as an amplitude and a phase, so it cannot be written as a completely positive real wavefunction in any basis.\n",
    "\n",
    "### Two qubits\n",
    "The two qubits are \n",
    "\\begin{equation}\n",
    "            |\\psi \\rangle = \\alpha |00\\rangle + \\beta | 01\\rangle + \\gamma |10\\rangle + \\delta 11\\rangle\n",
    "\\end{equation}\n",
    "where $\\alpha, \\beta, \\gamma, \\delta$ we want to approximate. The exact values used for this tutorial are in qubits_psi.txt, and are \n",
    "\\begin{equation}\n",
    "\\alpha =0.2860859781  + 0.0538594435 i \\\\\n",
    "\\beta = 0.3686925645 - 0.3022891852 i \\\\\n",
    "\\gamma = -0.1672402652 - 0.3528898162 i \\\\\n",
    "\\delta = -0.5658788296 - 0.4639198598 i\n",
    "\\end{equation}\n",
    "\n",
    "##### Code \n",
    "As before, we load the required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from qucumber.nn_states import ComplexWavefunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.data as data              \n",
    "import qucumber.utils.cplx as cplx              \n",
    "import qucumber.utils.unitaries as unitaries    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "The training data for a complex wavefunction consists of two file. Firstly, the basis which are measured must be listed. In our example, this is in \"qubits_train_bases.txt\" Secondly, we need the values of the measurements (0, 1) from measurement with those opterators. This is included in \"ubits_train.txt\"and we encourage the reader to take a look at both files to see the format. \n",
    "\n",
    "We also need a list of all the unique basis measurements from the \"quits_train_bases.txt\"file. This can be easily found with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XZ', 'YZ', 'ZX', 'ZY', 'ZZ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.loadtxt(\"qubits_train_bases.txt\", dtype=str)\n",
    "bases = np.unique(a, axis=0)\n",
    "bases = [''.join(bases[i,:]) for i in range(bases.shape[0])]\n",
    "bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the training data can be loaded with the utility function qucumber.utils.data.load_data(). This function simply read the .txt files, and converts them to torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'qubits_train.txt'\n",
    "train_bases_path = 'qubits_train_bases.txt'\n",
    "psi_path = 'qubits_psi.txt'\n",
    "train, psi, train_bases = data.load_data(train_path, psi_path, train_bases_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a **ComplexWavefunction** neural network state we need to create a dictionary that contains the unitaries that have been applied during the measurements. In our case this were the unitaries $\\mathbb{1}$, $H$, $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[[ 0.7071,  0.7071],\n",
       "          [ 0.7071, -0.7071]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]]], dtype=torch.float64),\n",
       " 'Y': tensor([[[ 0.7071,  0.0000],\n",
       "          [ 0.7071,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.7071],\n",
       "          [ 0.0000,  0.7071]]], dtype=torch.float64),\n",
       " 'Z': tensor([[[1., 0.],\n",
       "          [0., 1.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.]]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitary_dict = unitaries.create_dict()\n",
    "'''If you would like to add your own quantum gates from your experiment to \n",
    "   \"unitary_dict\", do:\n",
    "   unitary_dict = unitaries.create_dict(name='your_name', \n",
    "                                        unitary=torch.tensor([[real part], \n",
    "                                                              [imaginary part]], \n",
    "                                                             dtype=torch.double)\n",
    "                                                             \n",
    "   For example: \n",
    "   unitaries = unitary_library.create_dict(name='qucumber', \n",
    "                                           unitary=torch.tensor([ [[1.,0.],[0.,1.]] \n",
    "                                                                  [[0.,0.],[0.,0.]] ], \n",
    "                                                                dtype=torch.double))\n",
    "                                                                                             \n",
    "   By default, unitary_library.create_dict() contains the idenity matrix and the \n",
    "   hadamard and K gates with keys Z, X and Y, respectively.\n",
    "'''\n",
    "\n",
    "unitary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of visible units is equal to the number of qubits or the length of a training sample. The number of hidden units is set equal to the number of visible units for a neuron density of $\\alpha=1$.\n",
    "\n",
    "Again GPU computing is supported by setting  \"gpu = True\" in ComplexWavefunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state = ComplexWavefunction(num_visible=2, num_hidden=2, unitary_dict=unitary_dict, gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how the RBM is training, we will compute the full KL divergence and the fidelity between the true wavefunction of the system and the wavefunction the RBM reconstructs.\n",
    "This can be done by initializing the parameters of the **ComplexWavefunction** and the **MetricEvaluator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every  = 10\n",
    "\n",
    "nn_state.space = nn_state.generate_hilbert_space(2) \n",
    "callbacks      = [MetricEvaluator(100, {'Fidelity':ts.fidelity,'KL':ts.KL},target_psi=psi,bases=bases,\n",
    "                                  verbose=True, space=nn_state.space)]\n",
    "# The \"verbose=True\" argument will print the parameters in { } as a function of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing everything we can start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  13%|█▎        | 13/100 [00:29<03:15,  2.24s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-52a85a47c9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_bases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_bases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/wavefunction.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, pos_batch_size, neg_batch_size, k, lr, input_bases, progbar, time, callbacks, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mall_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear any cached gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/wavefunction.py\u001b[0m in \u001b[0;36mcompute_batch_gradients\u001b[0;34m(self, k, samples_batch, neg_batch, bases_batch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;31m# Positive phase: learning signal driven by the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m#                 (and bases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0mdata_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbases_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;31m# Accumulate amplitude RBM gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mgrad_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/complex_wavefunction.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, basis, sample)\u001b[0m\n\u001b[1;32m    217\u001b[0m             ]\n\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotated_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_sites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/complex_wavefunction.py\u001b[0m in \u001b[0;36mrotated_gradient\u001b[0;34m(self, basis, sites, sample)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Gradient on the current configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             grad_vp = [\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbm_am\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_energy_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbm_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_energy_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             ]\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/rbm/binary_rbm.py\u001b[0m in \u001b[0;36meffective_energy_gradient\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_h_given_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/rbm/binary_rbm.py\u001b[0m in \u001b[0;36mprob_h_given_v\u001b[0;34m(self, v, out)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         p = torch.addmm(\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         ).sigmoid_()\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_state.fit(train, epochs=100, pos_batch_size=50, neg_batch_size=10, lr =1e-2, k=2, input_bases=train_bases, progbar=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Training \n",
    "\n",
    "After the training we can calculate state fidelity, observables or sample from the complex wavefunction\n",
    "the same way we did from the real-positive wavefunction. However, one has to keep in mind that the sampling only works in the Z basis.\n",
    "\n",
    "To sample from a trained complex wavefunction we define the number of samples *num_samples* we want to draw and the number of contrastive divergence steps *CD*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2000\n",
    "CD          = 200\n",
    "\n",
    "samples = nn_state.sample(num_samples, CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also analogous to the positive real wavefunction we can save and load the RBM parameters and the newly generated samples using the *save* function within the ComplexWavefunction object and save additional quantities like e.g. *the samples* to the same file with *metadata*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save('saved_parameters.pkl', metadata={'Samples':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
