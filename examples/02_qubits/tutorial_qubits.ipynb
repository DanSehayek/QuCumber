{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of a complex wavefunction\n",
    "\n",
    "In this tutorial, a walkthrough of how to reconstruct a **complex** wavefunction via training a *Restricted Boltzmann Machine* (RBM), the neural network behind qucumber, will be presented. After the training process, this tutorial will also demonstrate how new data can be generated.\n",
    "\n",
    "\n",
    "## The wavefunction to be reconstructed\n",
    "The simple wavefunction below describing two qubits (coefficients stored in *qubits_psi.txt*) will be reconstructed.\n",
    "\n",
    "\\begin{equation}\n",
    "            \\vert\\psi \\rangle = \\alpha \\vert00\\rangle + \\beta \\vert 01\\rangle + \\gamma \\vert10\\rangle + \\delta \\vert11\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "where the exact values of $\\alpha, \\beta, \\gamma$ and $\\delta$ used for this tutorial are \n",
    "\n",
    "\\begin{align}\n",
    "\\alpha & = 0.2861  + 0.0539 i \\\\\n",
    "\\beta &  = 0.3687 - 0.3023 i \\\\\n",
    "\\gamma & = -0.1672 - 0.3529 i \\\\\n",
    "\\delta & = -0.5659 - 0.4639 i.\n",
    "\\end{align}\n",
    "\n",
    "Our example dataset, *qubits_train.txt*, comprises of 500 $\\sigma$ measurements made in various bases (X, Y and Z). A corresponding file containing the bases for each data point in *qubits_train.txt*, *qubits_bases.txt*, is also required. As per convention, spins are represented in binary notation with zero and one denoting spin-down and spin-up, respectively.\n",
    "\n",
    "## Using qucumber \n",
    "\n",
    "### Imports\n",
    "To begin the tutorial, we first import the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from qucumber.nn_states import ComplexWavefunction\n",
    "\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.unitaries as unitaries\n",
    "\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python class *ComplexWavefunction* contains generic properties of a RBM meant to reconstruct a complex wavefunction, the most notable one being the gradient function required for stochastic gradient descent.\n",
    "\n",
    "To instantiate a *ComplexWavefunction* object, one needs to specify the number of visible and hidden units in the RBM. The number of visible units, *num_visible*, is given by the size of the physical system, i.e. the number of spins or qubits (10 in our case), while the number of hidden units, *num_hidden*, can be varied to change the expressiveness of the neural network.\n",
    "\n",
    "**Note:** The optimal *num_hidden* : *num_visible* ratio will depend on the system. For the two-qubit wavefunction described above, good results are yielded when this ratio is 1.\n",
    "\n",
    "On top of needing the number of visible and hidden units, a *ComplexWavefunction* object requires the user to input a dictionary containing the unitary operators (2x2) that will be used to rotate the qubits in and out of the computational basis, Z, during the training process. The *unitaries* utility will take care of creating this dictionary.\n",
    "\n",
    "The *MetricEvaluator* class and *training_statistics* utility are built-in amenities that will allow the user to evaluate the training in real time. \n",
    "\n",
    "\n",
    "### Training\n",
    "To evaluate the training in real time, the fidelity between the true wavefunction of the system and the wavefunction that qucumber reconstructs, $\\vert\\langle\\psi\\vert\\psi_{RBM}\\rangle\\vert^2$, will be calculated along with the Kullback-Leibler (KL) divergence (the RBM's cost function). First, we need to load our training data and the true wavefunction of this system using the *data* utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path       = \"qubits_train.txt\"\n",
    "train_bases_path = \"qubits_train_bases.txt\"\n",
    "psi_path         = \"qubits_psi.txt\"\n",
    "bases_path       = \"qubits_bases.txt\"\n",
    "\n",
    "train_samples, true_psi, train_bases, bases = data.load_data(train_path, psi_path, train_bases_path, bases_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previouosly mentioned, a *ComplexWavefunction* object requires a dictionary that contains the unitariy operators that will be used to rotate the qubits in and out of the computational basis, Z, during the training process. In the case of the provided dataset, the unitaries required are the well-known $H$, and $K$ gates. The dictionary needed can be created with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary_dict = unitaries.create_dict()\n",
    "#unitary_dict = unitaries.create_dict(name='your_name', \n",
    "#                                     unitary=torch.tensor([[real part], \n",
    "#                                                           [imaginary part]], \n",
    "#                                                          dtype=torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wishes to add their own unitary operators from their experiment to *unitary_dict*, uncomment the block above. When *unitaries.create_dict()* is called, it will contain the identity, the $H$ and $K$ gates by default with the keys \"Z\", \"X\" and \"Y\", respectively.\n",
    "\n",
    "The number of visible units in the RBM is equal to the number of qubits. The number of hidden units will also be taken to be the number of visible units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = train_samples.shape[-1]\n",
    "nh = nv\n",
    "\n",
    "nn_state = ComplexWavefunction(num_visible=nv, num_hidden=nh, unitary_dict=unitary_dict)\n",
    "#nn_state = ComplexWavefunction(num_visible=nv, num_hidden=nh, unitary_dict=unitary_dict, gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, qucumber will attempt to run on a GPU if one is available (if one is not available, qucumber will default to CPU). If one wishes to run qucumber on a CPU, add the flag \"gpu = False\" in the *ComplexWavefunction* object instantiation (i.e. uncomment the line above). \n",
    "\n",
    "Now we can specify the hyperparameters of the training process. \n",
    "\n",
    "1. **epochs**: the total number of training cycles that will be performed (default = 100)\n",
    "2. **pos_batch_size**: the number of data points used in the positive phase of the gradient (default = 100)\n",
    "3. **neg_batch_size**: the number of data points used in the negative phase of the gradient (default = *pos_batch_size*)\n",
    "4. **k**: the number of contrastive divergence steps (default = 1)\n",
    "5. **lr**: the learning rate (default = 0.001)\n",
    "\n",
    "    **Note:** For more information on the hyperparameters above, we strongly encourage the user to read through our brief, but thorough theory document on RBMs. One does not have to specify these hyperparameters, as their default values will be used without the user overwriting them. It is recommended to keep with the default values until the user has a stronger grasp on what these hyperparameters mean. The quality and the computational efficiency of the training will highly depend on the choice of hyperparameters. As such, playing around with the hyperparameters is almost always necessary. \n",
    "    \n",
    "The two-qubit example in this tutorial should be extremely easy to train, regardless of the choice of hyperparameters. However, the hyperparameters below will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "pbs    = 50 # pos_batch_size\n",
    "nbs    = 10 # neg_batch_size\n",
    "lr     = 0.01\n",
    "k      = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the training in real time, we will call on the *MetricEvaluator* to calculate the training evaluators every 10 epochs. The *MetricEvaluator* requires the following arguments.\n",
    "\n",
    "1. **log_every**: the frequency of the training evaluators being calculated is controlled by the *log_every* argument (e.g. *log_every* = 200 means that the *MetricEvaluator* will update the user every 200 epochs)\n",
    "2. A dictionary of functions you would like to reference to evaluate the training (arguments required for these functions are keyword arguments placed after the dictionary)\n",
    "\n",
    "The following additional arguments are needed to calculate the fidelity and KL divergence in the *training_statistics* utility.\n",
    "\n",
    "- **target_psi** (the true wavefunction of the system)\n",
    "- **verbose** (a boolean for telling the *MetricEvaluator* to print the training evaluators)\n",
    "- **space** (the hilbert space of the system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every = 50\n",
    "nn_state.space = nn_state.generate_hilbert_space(nv)\n",
    "\n",
    "callbacks = [\n",
    "    MetricEvaluator(log_every, {\"Fidelity\": ts.fidelity, \"KL\": ts.KL}, target_psi=true_psi, bases=bases,\n",
    "                    verbose=True, space=nn_state.space)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin training. The *PositiveWavefunction* object has a property called *fit* which takes care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50\tFidelity = 0.754661\tKL = 0.134845\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35eae09740d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m nn_state.fit(train_samples, epochs=epochs, pos_batch_size=pbs, neg_batch_size=nbs, lr=lr, k=k, input_bases=train_bases,\n\u001b[0;32m----> 2\u001b[0;31m              callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/wavefunction.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, pos_batch_size, neg_batch_size, k, lr, input_bases, progbar, time, callbacks, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0mall_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear any cached gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/wavefunction.py\u001b[0m in \u001b[0;36mcompute_batch_gradients\u001b[0;34m(self, k, samples_batch, neg_batch, bases_batch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;31m# Positive phase: learning signal driven by the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m#                 (and bases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0mdata_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbases_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;31m# Accumulate amplitude RBM gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mgrad_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/complex_wavefunction.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, basis, sample)\u001b[0m\n\u001b[1;32m    219\u001b[0m             ]\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotated_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_sites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/nn_states/complex_wavefunction.py\u001b[0m in \u001b[0;36mrotated_gradient\u001b[0;34m(self, basis, sites, sample)\u001b[0m\n\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             rotated_grad[1] += cplx.scalar_mult(\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mUpsi_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcplx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             )\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/testing/lib/python3.6/site-packages/qucumber-0.2.1-py3.6.egg/qucumber/utils/cplx.py\u001b[0m in \u001b[0;36mscalar_mult\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_state.fit(train_samples, epochs=epochs, pos_batch_size=pbs, neg_batch_size=nbs, lr=lr, k=k, input_bases=train_bases,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that all the code past the fourth code block of this notebook is not needed for the training process. One could have just ran *nn_state.fit(train_samples)* and just used the default hyperparameters and no training evaluators.\n",
    "\n",
    "### Post-training\n",
    "#### Saving and loading parameters\n",
    "\n",
    "At the end of the training process, the network parameters (the weights, visible biases and hidden biases) are stored in the *ComplexWavefunction* object. One can save them to a pickle file, which will be called *saved_params*, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save(\"saved_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the weights, visible biases and hidden biases as torch tensors with the following keys: \"weights\", \"visible_bias\", \"hidden_bias\". One can then load a new *ComplexWavefunction* object with saved parameters with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state2 = ComplexWavefunction.autoload(\"saved_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new data\n",
    "\n",
    "At the end of the training process, the user can then generate new data. A *ComplexWavefunction* object has a property called *sample* that takes in the following arguments.\n",
    "\n",
    "1. **k**: the number of Gibbs steps to perform to generate the new samples\n",
    "2. **num_samples**: the number of new data points to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = nn_state.sample(k=10, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new data can also be saved (along with the RBM parameters) with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_state.save(\"saved_params_and_new_data.pt\", metadata={\"samples\":new_samples})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*metadata* is a disctionary that is an optional argument in the *PositiveWavefunction*'s *save* function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
